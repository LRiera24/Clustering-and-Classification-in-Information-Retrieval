% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{color}

\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage[noend]{algpseudocode}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[spanish]{babel}

% Keywords command
\providecommand{\keywords}[1]
{
	\small	
	\textbf{\textit{Palabras clave --- }} #1
}

\begin{document}
\markboth{Sistemas de Recuperaci\'on de Informaci\'on}{Aplicaciones del agrupamiento y de la clasificación en la RI en la Web}
\thispagestyle{empty}
\begin{flushleft}
\LARGE\bfseries Sistemas de Recuperaci\'on de Informaci\'on\\[2cm]
\end{flushleft}
\rule{\textwidth}{1pt}
\vspace{2pt}
\begin{flushright}
\Huge
\begin{tabular}{@{}l}
Aplicaciones del\\
agrupamiento y\\ 
de la clasificación\\ 
en la recuperaci\'on\\ 
de informaci\'on\\
en la Web\\[6pt]
\end{tabular}
\end{flushright}
\rule{\textwidth}{1pt}
\vfill
\begin{flushleft}
\large\itshape
\begin{tabular}{@{}l}
{\large\upshape\bfseries Autores}\\[8pt]
Laura Victoria Riera P\'erez\\[5pt]
Marcos Manuel Tirador del Riego
\end{tabular}
\end{flushleft}

\newpage
\pagenumbering{gobble}
	\begin{abstract}
	La recuperación de información (RI) es un subcampo de la ciencia de la información relacionado con la representación, el almacenamiento, acceso y recuperación de la información. Las áreas de investigación actuales dentro del campo de RI incluyen búsquedas y consultas, clasificación de resultados de búsqueda, navegación y búsqueda de información, optimización de la representación de información, almacenamiento, clasificación de documentos y agrupamiento. A partir de la expansión y consolidación de Internet, como medio principal de comunicación electrónica de datos, se ha puesto a disposición de casi toda la humanidad una importante cantidad de información de todo tipo. A los efectos de aprovechar todo
	este potencial de información, es necesario poseer accesos que permitan que la tarea de recuperación sea eficiente y efectiva a la vez que se mejore la experiencia e interfaz de usuario. El objetivo principal de este artículo es comprender el uso de la agrupación y clasificaci\'on de documentos para mejorar la recuperación de su información. De cada uno se muestran las caracter\'isticas principales, medidas de similitud y evaluaci\'on, ventajas, desventajas y las aplicaciones en la RI (en particular en la web). Adem\'as se analizan los algoritmos K-Means y HAC de agrupaci\'on; y el Naive Bayes y K-Nearest-Neighbours de clasificaci\'on. 
	
	\vspace{1em}
	\keywords{agrupamiento \textbf{$\cdot$} agrupamiento particionado \textbf{$\cdot$} agrupamiento jer\'arquico \textbf{$\cdot$} clasificaci\'on \textbf{$\cdot$} web \textbf{$\cdot$} Recuperaci\'on de Informaci\'on (RI)}
\end{abstract} 
\thispagestyle{empty}


\newpage
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}

\newpage
\pagenumbering{arabic}

\section{Introducci\'on}

La categorizaci\'on es uno de los procesos fundamentales en la ciencia. Es importante reconocer que la categorizaci\'on trasciende el intelecto humano y es una propiedad fundamental de los organismos vivos. Los intentos de desarrollar t\'ecnicas para separar observaciones de forma autom\'atica en grupos o clases requiere de la cuantificaci\'on de la similitud. Es por ello que la habilidad de reconocer dos objetos como m\'as similares entre ellos que a un tercero de seguro debe haber estado en los ancestros de la raza humana. En muchos trabajos clasificatorios manuales es poco pr\'actico estimar similutudes taxon\'omicas entre objetos de una muestra.

La clasificaci\'on y el agrupamiento son dos m\'etodos de aprendizaje de m\'aquinas usados para categorizar colecciones de datos. Ambos m\'etodos tienen como objetivo separar las observaciones en grupos que compartan caracter\'isticas similares mientras se maximice la separaci\'on entre grupos que se diferencien mucho entre s\'i. Debido a que comparten un objetivo estos dos m\'etodos suelen confundirse, sin embargo se diferencian en la filosof\'ia utilizada para la clasificaci\'on de los objetos. En clasificaci\'on los objetos se asignan a clases prefijadas de antemano de acuerdo a caracter\'isticas perceptibles de los objetos. En cambio en agrupamiento la separaci\'on en grupos ocurre de forma natural de acuerdo a similitudes entre los objetos que muchas veces el humano no es capaz de interpretar. Debido a la necesidad de un experto humano la clasificaci\'on entra dentro de la rama del aprendizaje supervisado, mientras que los algoritmos de agrupamiento son no supervisados.

\section{Agrupamiento}

\begin{flushright}
	\scriptsize*(Tomado de \cite{B1})
	\normalsize
\end{flushright}

Los algoritmos de agrupamiento, como su nombre lo indica, agrupan un conjunto de documentos en subconjuntos o clústeres. Son utilizados para generar una estructura de categorías que se ajuste a un conjunto de observaciones. 

Los grupos formados deben tener un alto grado de asociación entre los documentos de un mismo grupo, es decir, deben ser lo m\'as similares posibles, y un bajo grado entre miembros de diferentes grupos. 

Es la forma más común de \textit{aprendizaje no supervisado}, es decir no hay ningún experto humano que haya asignado documentos a clases. Es la distribución y composición de los datos lo que determinará la pertenencia al clúster. 

Si bien el agrupamiento a veces es denominado clasificación automática, esto no es estrictamente exacto ya que las clases formadas no se conocen antes de tratamiento, como implica la clasificación, sino que se definen por los elementos que se les asignan.

La entrada clave para un algoritmo de agrupamiento es la medida de distancia. Diferentes medidas de distancia dan lugar a diferentes agrupamientos. En el agrupamiento de documentos, la medida de distancia es a menudo también la distancia euclidiana. 

\vspace{1em}
\textbf{Hip\'otesis de agrupamiento:} \textit{Los documentos en el mismo grupo se comportan de manera similar con respecto a la relevancia para las necesidades de información.}
	
\vspace{0.3em}
La hipótesis establece que si hay un documento de un grupo que es relevante a una solicitud de búsqueda, entonces es probable que otros documentos del mismo clúster también sean relevantes. 
\vspace{1em}

Los algoritmos de agrupamiento pueden ser clasificados seg\'un la pertenencia a los grupos en \textit{agrupamiento exclusivo o fuerte} (hard clustering) donde cada documento es miembro de exactamente un grupo; o \textit{agrupamiento difuso o suave} (soft clustering) donde un documento tiene membresía fraccionaria en varios grupos.

Tambi\'en pueden clasificarse seg\'un el tipo de estructura impuesta sobre los datos como \textit{agrupamiento particionado o plano} (flat clustering) o \textit{agrupamiento jer\'arquico} (hierarchical clustering).

\subsection{Medidas de similitud}

\begin{flushright}
	\scriptsize*(Tomado de \cite{B3})
	\normalsize
\end{flushright}

Sean $ d_{i} $ el documento $ i $ del corpus y $ w_{ik} $ el peso del t\'ermino $ k $ de un total $ N $ en el documento $ i $ .

\begin{itemize}
	\item \textbf{Coeficiente de Dice:}
	\begin{center}
		$ S_{d_{i}, d_{j}} = \dfrac{2 \sum_{k=1}^{N} (w_{ik}w_{jk})}{\sum_{k=1}^{N}w_{ik}^{2} + \sum_{k=1}^{N}w_{jk}^{2}} $
	\end{center}
	\item \textbf{Coeficiente de Dice para documentos representados como vectores binarios:}
	\begin{center}
		$ S_{d_{i}, d_{j}} = \dfrac{2C}{A+B} $
	\end{center}
	donde C es la cantidad de t\'erminos que los documentos $ d_{i} $ y $ d_{j} $ tienen en com\'un, y A y B son la cantidad total de t\'erminos en $ d_{i} $ y $ d_{j} $ respectivamente.
	\item \textbf{Coeficiente de Jaccard:}
	\begin{center}
		$ S_{d_{i}, d_{j}} = \dfrac{\sum_{k=1}^{N} (w_{ik}w_{jk})}{\sum_{k=1}^{N}w_{ik}^{2} + \sum_{k=1}^{max}w_{jk}^{2} - \sum_{k=1}^{N} (w_{ik}w_{jk})} $
	\end{center}
	\item \textbf{Coeficiente del coseno:}
	\begin{center}
		$ S_{d_{i}, d_{j}} = \dfrac{\sum_{k=1}^{N} (w_{ik}w_{jk})}{\sqrt{\sum_{k=1}^{N}w_{ik}^{2} \sum_{k=1}^{N}w_{jk}^{2}}} $
	\end{center}
\end{itemize}

\subsection{Medidas de evaluaci\'on} 
	\begin{flushright}
		\scriptsize*(Tomado de \cite{B1})
	\end{flushright}
	\normalsize
	\begin{itemize}
	\item \textbf{Pureza:} Para calcular la pureza, cada grupo se asigna a la clase que es más frecuente en el grupo, y luego se mide la precisión de esta asignación contando el número de documentos correctamente asignados y dividiendo por N.
	
	\begin{center}
		$purity(\Omega, \mathbb{C}) = \dfrac{1}{N} \sum_{k}max_{j} |\omega_{k} \cap c_{j}| $
	\end{center}
	
	donde $ \Omega = \{\omega_{1}, \omega_{2}, ... , \omega_{K}\} $ es el conjunto de cl\'usters y $ C = \{c_{1}, c_{2}, ... , c_{J}\} $ es el conjunto de clases. Se interpreta $ \omega_{k} $ como el conjunto de documentos en el cl\'uster y $ c_{j} $ como el conjunto de documentos que pertenecen a esa clase.
	
	Malos agrupamientos tienen valores de pureza cercanos a 0, mientras que un agrupamiento perfecto tiene una pureza 1.
	
	La alta pureza es fácil de lograr cuando el número de grupos es grande, en particular, la pureza es 1 si cada documento tiene su propio grupo. Sin embargo en este caso tendr\'iamos la mayor cantidad de grupos posibles, por lo que nuestra soluci\'on no tendr\'ia calidad.
	
	\vspace{1em}
	\item \textbf{Información mutua normalizada:} Una medida que nos permite hacer la compensaci\'on entre calidad y pureza es la información mutua normalizada o $ NMI $ por sus siglas en ingl\'es.
	
	\begin{center}
		$ NMI(\Omega, \mathbb{C}) = \dfrac{I(\Omega, \mathbb{C})}{[H(\Omega) + H(\mathbb{C})]/2}
		$
	\end{center}
	
	I es la informaci\'on mutua:
	\begin{center}
		$ I(\Omega, \mathbb{C}) = \sum_{k}\sum_{j}P(\omega_{k} \cap c_{j})\log\frac{P(\omega_{k} \cap c_{j})}{P(\omega_{k})P(c_{j})} $
	\end{center}
	donde $ P(\omega_{k}) $, $ P(c_{j}) $ y $ P(\omega_{k} \cap c_{j}) $ son las probabilidades de que un documento sea del cl\'uster $ \omega_{k} $, clase $ c_{j} $, y est\'e en la intersección de $ \omega_{k} $ y $ c_{j} $, respectivamente.
	
	\vspace{0.5em}
	H es la entropía (mide la incertidumbre de una fuente de información)
	
	\begin{center}
		$ H(\Omega) = -\sum_{k}P(\omega_{k}) \log P(\omega_{k}) $
	\end{center}
	
	Se alcanza la máxima información mutua para un agrupamiento exacto que perfectamente recrea las clases pero también si los grupos se subdividen en cl\'usteres más pequeños. 
	
	En particular, en un agrupamiento con $ K = N $ los grupos de documentos tienen $ MI $ máximo. Entonces $ MI $ tiene el mismo problema que la pureza, no penaliza cardinalidades grandes.
	
	La normalización por el denominador $ [H(\Omega) + H(\mathbb{C})]/2 $ soluciona este problema ya que la entropía tiende a aumentar con el número de cl\'usteres, alcanzando su $ log N $ máximo para $ K = N $, lo que asegura que $ NMI $ es bajo para una gran cantidad de cl\'usteres.
	
	\vspace{1em}
	\item \textbf{\'Indice de Rand (Rand Index)}: Se asignan dos documentos al mismo clúster si y sólo si son similares. Una decisión positiva verdadera (TP) asigna
	dos documentos similares al mismo grupo, una decisión negativa verdadera (TN) asigna dos documentos diferentes a diferentes grupos. Hay dos tipos
	de errores que podemos cometer. Una decisión de falso positivo (FP) asigna dos documentos no similarres al mismo clúster. Una decisión de falso negativo (FN) asigna
	dos documentos similares a diferentes agrupaciones. El índice de Rand (IR) mide el porcentaje de decisiones que son correctas (precisión).
	
	\begin{center}
		$ RI = \dfrac{TP + TN}{TP + FP + FN + TN} $
	\end{center}
	
	El índice de Rand otorga el mismo peso a los falsos positivos y falsos negativos. Separar documentos similares a veces es peor que poner pares de dos documentos no similares en el mismo grupo.
	
	\vspace{1em}
	\item \textbf{Medida F:} Podemos usar la medida F, vista anteriormente en conferencia, para penalizar los falsos negativos más fuertemente que los falsos positivos seleccionando un valor $ \beta > 1 $, dando así más peso al recobrado.
	\begin{center}
		$ P = \dfrac{TP}{TP + FP}  $ \hspace{2em}
		$ R = \dfrac{TP}{TP + FN} $
		
		\vspace{1em}
		$ F_{\beta} = \dfrac{(\beta^{2} + 1)PR}{\beta^{2}P + R} $
	\end{center}
\end{itemize}

\subsection{Agrupamiento particionado}

El \textit{agrupamiento particionado} crea un conjunto  de clústeres sin ninguna estructura explícita que los relacione entre sí. 

\vspace{1em}
\textbf{Algoritmo K-means}
\begin{flushright}
	\scriptsize*(Tomado de \cite{B1})
	\normalsize
\end{flushright}
\vspace{0.5em}

	Es el algoritmo de agrupamiento plano más importante. Su \textit{objetivo} es minimizar la distancia euclidiana al cuadrado promedio entre los documentos y el centro de sus cl\'usteres. 
	
	El centro de un cl\'uster se define como la media o centroide $\mu$ de los documentos en un grupo $\omega$:
	
	\begin{center}
		$ \overrightarrow{\mu}(\omega) \leftarrow \dfrac{1}{|\omega_{k}|} \sum_{\overrightarrow{x} \in \omega_{k}} \overrightarrow{x} $
	\end{center}
	
	Se asume que los documentos se representan como vectores de longitud normalizada  en un espacio de valor real de la manera habitual.  
	
	Una medida de qué tan bien los centroides representan a los miembros de su cl\'uster es la suma residual de cuadrados o RSS, que es la distancia al cuadrado de cada vector desde su centroide sumado sobre todos los vectores:
	
	\begin{center}
		$ RSS_{k} = \sum_{\overrightarrow{x} \in \omega_{k}}|\overrightarrow{x} - \overrightarrow{\mu}(\omega_{k})|^2$
	
	\vspace{1em}
	$ RSS = \sum_{k=1}^{K} RSS_{k} $
	\end{center}

	RSS es entonces la \textit{función objetivo} en K-means y nuestro objetivo es minimizarla.
	
	\begin{algorithm}
		\caption{K-Means}
		\begin{algorithmic}[1]
			\Require{$\{\overrightarrow{x_{1}}, ... , \overrightarrow{x_{N}}\}, K $} 
			%		 			\Ensure{Trained.}
			\State{$(\overrightarrow{s_{1}}, . . . , \overrightarrow{s_{K}}) \leftarrow \textbf{SelectRandomSeeds}(\{\overrightarrow{x_{1}}, ... , \overrightarrow{x_{N}}\}, K)$}
			\For{$k \leftarrow 1$ \textbf{to} $K$}
			\State{$ \overrightarrow{\mu_{k}} \leftarrow \overrightarrow{s_{k}} $}
			\EndFor
			\While{stopping criterion has not been met}
				\For{$k \leftarrow 1$ \textbf{to} $K$}
				\State{$ \omega_{k} \leftarrow \{\} $}
				\EndFor
				\For{$n \leftarrow 1$ \textbf{to} $N$}
				\State{$ j \leftarrow \argmin_{j'}|\overrightarrow{\mu_{j'}}-\overrightarrow{x_{n}}| $}
				\State{$ \mu_{j} \leftarrow \omega_{j} \cup \{\overrightarrow{x_{n}}\} $ \hspace{2em} (reassignment of vectors) }
				\EndFor
				\For{$k \leftarrow 1$ \textbf{to} $K$}
				\State{$ \overrightarrow{\mu_{k}} \leftarrow \dfrac{1}{|\omega_{k}|} \sum_{\overrightarrow{x} \in \omega_{k}} \overrightarrow{x} $ \hspace{2em} (recomputation of centroids)}
				\EndFor
			\EndWhile
			\State{\Return {$\{\overrightarrow{\mu_{1}}, ... , \overrightarrow{\mu_{N}}\}$}}
		\end{algorithmic}
	\end{algorithm}

El primer paso de esta implementaci\'on de K-means es seleccionar al azar como centros iniciales de los cl\'usteres a K documentos, estas son las semillas. Luego, el algoritmo mueve los centros de los grupos en el espacio para minimizar el RSS. Este proceso se repite de manera iterativa hasta que se cumpla un criterio de parada.

Opciones para la \textit{elecci\'on de las semillas}:
\begin{flushright}
	\scriptsize*(Tomado de \cite{B2})
	\normalsize
\end{flushright}
\begin{itemize}
	\item Escoger al azar.
	\item Calcular la media $ m $ de todos los datos y generar k centros iniciales agregando un pequeño vector aleatorio a la media ($ m \pm \varsigma $).
	\item Utilizar otro método como un algoritmo de agrupamiento jerárquico en un subconjunto de los objetos.
\end{itemize}

Algunos \textit{criterios de parada}:
\begin{flushright}
	\scriptsize*(Tomado de \cite{B1})
	\normalsize
\end{flushright}
\begin{itemize}
	\item Cuando se ha completado un número fijo de iteraciones I. Esta condición limita el tiempo de ejecución del algoritmo de agrupamiento, pero en algunos casos la calidad de el agrupamiento será deficiente debido a un número insuficiente de iteraciones.
	\item Cuando la asignación de documentos a grupos no cambia entre iteraciones. Excepto en los casos con un m\'inimo local malo, esto produce un buen agrupamiento, pero los tiempos de ejecución pueden ser demasiado largos.
	\item Cuando los centroides no cambian entre iteraciones.
	\item Cuando RSS cae por debajo de un umbral. Este criterio asegura que el agrupamiento tiene la calidad deseada después de la terminación. En la práctica, es necesario tener adem\'as un límite en el número de iteraciones para garantizar terminación.
\end{itemize}

\subsection{Agrupamiento jer\'arquico}

El \textit{agrupamiento jerárquico} produce una jerarquía, una estructura que es más informativa que el conjunto no estructurado de clústeres devuelto por el agrupamiento particionado, no requiere que especifiquemos previamente el número de grupos y la mayoría son deterministas, sin embargo son m\'as ineficientes que los particionados. En una representación gráfica los elementos quedan anidados en jerarquías con forma de árbol.

Los algoritmos de agrupamiento jerárquico pueden tener dos enfoques: de arriba hacia abajo (top-down) llamados de \textit{agrupamiento jer\'arquico aglomerativo} o de abajo hacia arriba (bottom-up) conocidos como de \textit{agrupamiento jer\'arquico divisivo}. 

\subsubsection{Agrupamiento jer\'arquico aglomerativo}
\textcolor{white}{.}

\vspace{0.5em}
Los algoritmos de abajo hacia arriba tratan cada documento como un clúster único desde el principio y luego fusionan (o aglomeran) sucesivamente pares de grupos hasta que todos los grupos se han fusionado en uno solo que contiene todos los documentos. 
Es por esto que se denomina agrupamiento jerárquico aglomerativo o HAC por sus siglas en ingl\'es. 

Toman decisiones basadas en patrones locales sin tener inicialmente en cuenta la distribución global. Estas decisiones tempranas no se pueden deshacer.

\vspace{1em}
\textbf{Medidas de similitud para cl\'usteres en HAC}

\begin{flushright}
	\scriptsize*(Tomado de \cite{B1} y \cite{B3})
	\normalsize
\end{flushright}

\begin{itemize}
	\item \textbf{Agrupamiento por enlazamiento \'unico} (Single link clustering): La similitud entre dos cl\'usters es la similitud de los dos objetos más cercanos entre ellos (mayor similitud).
	
	\begin{center}
		$ sim(\omega_{i}, \omega_{j}) = max_{\overrightarrow{x} \in \omega_{i}, \overrightarrow{y} \in \omega_{j}} SIM(\overrightarrow{x}, \overrightarrow{y})$
	\end{center}
	
	\vspace{1em}
	\item \textbf{Agrupamiento por enlazamiento completo} (complete link clustering): La similitud entre dos cl\'usters es la similitud de los dos objetos más alejados entre ellos (menor similitud). 
	
	\begin{center}
		$ sim(\omega_{i}, \omega_{j}) = min_{\overrightarrow{x} \in \omega_{i}, \overrightarrow{y} \in \omega_{j}} SIM(\overrightarrow{x}, \overrightarrow{y})$
	\end{center}
	
	\vspace{1em}
	\item \textbf{Agrupamiento aglomerativo por promedio de grupo} (group-average agglomerative clustering): El agrupamiento aglomerativo por promedio de grupo o GAAC por sus siglas en ingl\'es, calcula la similitud promedio SIM-GA de todos los pares de documentos, incluidos los pares del mismo grupo (las auto-similitudes no están incluidas en el promedio).
	
	\begin{center}
	\footnotesize
	$ SIM-GA(\omega_{i}, \omega_{j}) = \dfrac{1}{(N_{i} + N_{j})(N_{i} + N_{j} - 1)} \sum_{\overrightarrow{x} \in \omega_{i} \cup \omega_{j}} \sum_{\overrightarrow{y} \in \omega_{i} \cup \omega_{j}, \overrightarrow{x} \neq \overrightarrow{y}} \overrightarrow{x} \cdot \overrightarrow{y} $
	\end{center}
	
	Este evalúa la calidad de un clúster basada en todas las similitudes entre documentos, evitando así castigar valores extremos como en los criterios de enlace único y enlace completo, que establecen la similitud del cl\'uster con la similitud de un solo par de documentos.
	
	\vspace{1em}
	\item \textbf{Agrupamiento por centroide} (centroid clustering): La similitud de dos cl\'usters est\'a definida como la similitud de sus centroides

		\begin{align}
			sim(\omega_{i}, \omega_{j}) &= \mu(\omega_{i})\cdot\mu(\omega_{j}) \nonumber\\
			&= \left(\dfrac{1}{N_{i}}\sum_{\overrightarrow{x} \in \omega_{i}}\overrightarrow{x}\right) \cdot \left(\dfrac{1}{N_{j}}\sum_{\overrightarrow{y} \in \omega_{i}}\overrightarrow{y}\right) \nonumber\\
			& = \dfrac{1}{N_{i}N_{j}} \sum_{\overrightarrow{x} \in \omega_{i}}\sum_{\overrightarrow{y} \in \omega_{j}}\overrightarrow{x}\cdot\overrightarrow{y} \nonumber
		\end{align}
	
	A diferencia de GAAC, el agrupamiento por centroide excluye el c\'alculo de pares del mismo cl\'uster.
\end{itemize}

\vspace{1em}
\textbf{Algoritmo HAC}
\begin{flushright}
	\scriptsize*(Tomado de \cite{B1})
	\normalsize
\end{flushright}

Dado un conjunto de N elementos a agrupar, el proceso básico del agrupamiento jerárquico es:
\begin{enumerate}
	\item Se comienza con N cl\'usteres, resultado de asignar cada elemento al suyo propio. Se computa la matriz C de similitud de N×N.
	
	\item Se halla la similitud entre los pares de cl\'usteres con la medida deseada.
	
	\item Se toma el par más similar de clústeres y se combinan en un único clúster.
	
	\item Se calculan las similitudes entre el nuevo clúster y cada uno de los cl\'usteres antiguos.
	
	\item Se repiten los pasos 3 y 4 hasta que todos los elementos estén agrupados en un solo grupo de tamaño N.
\end{enumerate}

\begin{algorithm}
	\caption{HAC}
	\begin{algorithmic}[1]
		\Require{$\{d_{1}, ... , d_{N}\} $} 
		%		 			\Ensure{Trained.}
		\For{$n \leftarrow 1$ \textbf{to} $N$}
		\For{$i \leftarrow 1$ \textbf{to} $N$}
		\State{$ C[n][i] \leftarrow SIM(d_{n}, d_{i}) $}
		\State{$ I[n] \leftarrow 1 $ (keeps track of active clusters)}
		\EndFor
		\State{$ A \leftarrow [] $ (assembles clustering as a sequence of merges)}
		\EndFor
		\For{$k \leftarrow 1$ \textbf{to} $N-1$}
		\State{$ <i,m> \leftarrow \argmax_{<i,m>:i \neq \wedge I[i] = 1 \wedge I[m] = 1} C[i][m] $}
		\State{$ A.APPEND(<i,m>) $ (store merge)}
		\For{$j \leftarrow 1$ \textbf{to} $N$}
		\State{$ C[i][j] \leftarrow SIM(i,m,j)$}
		\State{$ C[j][i] \leftarrow SIM(i,m,j)$}
		\EndFor
		\State{$ I[m] \leftarrow 0 $ (deactivate cluster)}
		\EndFor
		\State{\Return {A}}
	\end{algorithmic}
\end{algorithm}

Notas: 
\begin{itemize}
	\item En cada iteración, los dos clústeres m\'as similares se fusionan y las filas y columnas del clúster fusionado i en C se actualizan.
	\item El agrupamiento se almacena como una lista de fusiones en A.
	\item I indica qué clústeres aún están disponibles para fusionarse. 
	\item La función SIM(i, m, j) calcula la similitud del grupo j con la fusión de los grupos i y M.
\end{itemize}

Una suposición fundamental en HAC es que la operación de fusión es mon\'otona, es decir si $ s_{1}, s_{2}, . . . , s_{K-1}$ son las similitudes de combinación de las fusiones sucesivas de un HAC, entonces se cumple $ s_{1} \geq s_{2} \geq . . . \geq s_{K-1}$.

El agrupamiento jerárquico no requiere un número predeterminado de clústeres. Sin embargo, en algunas aplicaciones queremos una partición de clústeres disjuntos como en el agrupamiento particionado. En esos casos, la jerarquía debe cortarse en algún momento. Se pueden utilizar varios criterios para determinar el punto de corte:
\begin{itemize}
	\item Cortar a un nivel de similitud preespecificado.
	\item  Aplicar la ecuación:
	\begin{center}
		$K = \argmin_{K'}[RSS(K') + \lambda K'] $
	\end{center}
	donde $ K' $ se refiere al corte de la jerarquía que resulta en $ K' $ clusters, $ RSS $ es la suma residual de cuadrados y $\lambda$ es una penalización por cada grupo adicional.
	\item Al igual que en el agrupamiento particionado, también se puede preespecificar el número de agrupamientos $ K $ y seleccionar el punto de corte que produce $ K $ cl\'usteres.
\end{itemize}

\subsubsection{Agrupamiento jer\'arquico divisivo}

\textcolor{white}{.}
\begin{flushright}
	\scriptsize*(Tomado de \cite{B1})
	\normalsize
\end{flushright}

\vspace{0.5em}
Los algoritmos de arriba hacia abajo comienzan con todos los documentos en un grupo. El clúster se divide utilizando un algoritmo de agrupamiento particionado. Este procedimiento se aplica recursivamente hasta que cada documento está en su
propio clúster.

A pesar de necesitar un segundo algoritmo de agrupamiento particionado como una subrutina, tiene la ventaja de ser más eficiente si no generamos una jerarquía completa hasta las hojas de documentos individuales. Para un número fijo de niveles, y utilizando un algoritmo particionado eficiente como K-means, los algoritmos divisivos son lineales en el número de documentos y clústeres.

Adem\'as se beneficia de la información completa sobre la distribución global al tomar decisiones de partición de alto nivel.

\subsection{Ventajas} 

\begin{itemize}
	\item No es necesario identificar las clases antes del procesamiento por lo que no se debe contar con expertos para este fin.
	
	\item Es útil para proporcionar estructura en grandes conjuntos de datos multivariados.
	
	\item Se ha descrito como una herramienta de descubrimiento porque tiene el potencial para revelar relaciones previamente no detectadas basadas en datos complejos.
	
	\item Debido a su amplia aplicación en dis\'imiles campos, cuenta el apoyo de una serie de paquetes de software, a menudo disponibles en la informática académica y otros entornos, por lo que se facilita su utilizaci\'on.
\end{itemize}

\subsection{Desventajas} 
	\begin{itemize}
		\item No se tiene una idea exacta de las clases creadas.
		\item No recibe retroalimentaci\'on.
	\end{itemize}

\subsection{Ejemplos de aplicaci\'on} 
El agrupamiento es una técnica importante para descubrir subregiones o subespacios relativamente densos de una distribución de datos multidimensional. Se ha utilizado en la recuperación de información para muchos propósitos diferentes, como la expansión de consultas, la agrupación e indexación de documentos y la visualización de resultados de búsqueda. Permiten mejorar interfaz y experiencia de usuario y proporcionar una mayor eficacia o eficiencia del sistema de búsqueda. 

A continuaci\'on se describen con m\'as detalle algunas de las aplicaciones m\'as importantes:
\begin{flushright}
	\scriptsize*(Tomado de \cite{B1})
	\normalsize
\end{flushright} 
\begin{itemize}
	\item Agrupamiento de resultados de búsqueda (Search result clustering): La presentación predeterminada de los resultados de búsqueda (documentos devueltos en respuesta a una consulta) en la recuperación de información es una lista sencilla. Los usuarios escanean la lista de arriba a abajo hasta que encuentran la información que buscan. 
	
	En su lugar, en la agrupación en clústeres de resultados de búsqueda los documentos similares aparecen juntos, siendo más fácil escanear algunos grupos coherentes que muchos documentos individuales. Esto es particularmente útil si un término de búsqueda tiene diferentes significados.
	
	\item Dispersión-recopilación (Scatter-Gather): Su objetivo es tambi\'en una mejor interfaz de usuario. Este agrupa toda la colección para obtener grupos de documentos que el usuario puede seleccionar o reunir manualmente. Los grupos seleccionados se fusionan y el conjunto resultante se vuelve a agrupar. Este proceso se repite hasta que se encuentre un grupo de interés.
	
	La navegación basada en la agrupaci\'on de clústeres es una alternativa interesante a la búsqueda de palabras clave a la información estándar paradigma de recuperación de información. Esto es especialmente cierto en escenarios donde los usuarios prefieren navegar en lugar de buscar porque no están seguros de qué búsqueda términos a utilizar.
	
	\item Modelado de lenguaje (Language modeling): Explota directamente la hipótesis del agrupamiento para mejorar los resultados de búsqueda, basado en una agrupación de toda la colección. Usamos un índice invertido estándar para identificar un conjunto inicial de documentos que coincide con la consulta, pero luego agregamos otros documentos de los mismos grupos incluso si tienen poca similitud con la consulta. 
	
 	Para evitar problemas de datos escasos en el modelado lenguaje enfocado a RI, el modelo de documento d se puede interpolar con un modelo de colección. Pero la colección contiene muchos documentos con términos atípicos de d. Al reemplazar el modelo de colección con un modelo derivado de grupo de d, obtenemos estimaciones más precisas de las probabilidades de ocurrencia de términos en d.
	
	\item Recuperaci\'on basada en cl\'usteres (Cluster-based retrieval): La agrupación también puede acelerar la búsqueda. 
	
	La búsqueda en el modelo de espacio vectorial equivale a encontrar los vecinos más cercanos a la consulta. El índice invertido admite la búsqueda rápida del vecino más cercano para la configuración estándar de RI. Sin embargo, a veces es posible que no podamos usar un índice invertido de manera eficiente. En tales casos, podríamos calcular la similitud de la consulta con cada documento, pero esto es lento. La hipótesis de agrupamiento ofrece una alternativa: encontrar los cl\'usteres que están más cerca de la consulta y sólo considerar los documentos de estos. Como hay muchos menos clústeres que documentos, se disminuye grandemente el espacio de b\'usqueda, y encontrar el clúster más cercano es rápido. Adem\'as los elementos que coinciden con una consulta son similares entre sí, por lo que tienden a estar en los mismos cl\'usteres, de esta forma la calidad no disminuye en gran medida.
\end{itemize}

\section{Clasificaci\'on}

	El problema de clasificaci\'on en sentido general consiste en determinar dentro un conjunto de clases a cu\'al de ellas pertenece un objeto dado. En el marco de este documento estamos interesados en estudiar la clasificaci\'on de textos. 
	
%	La forma m\'as simple de clasificaci\'on de un conjunto de textos es la denominada clasificaci\'on en dos clases. Dichas clases est\'an determinadas por un t\'opico espec\'ifico y ser\'ian: \emph{documentos sobre dicho tema } y \emph{documentos no relacionados con el tema}. Este tipo de clasificaci\'on en ocasiones es llamada \emph{filtrado}. 
%	
	La forma m\'as antigua de llevar a cabo la clasificaci\'on es manualmente. Por ejemplo, los bibliotecarios clasifican los libros de acuerdo a ciertos criterios, de modo que encontrar una informaci\'on buscada no resulte una tarea de gigante dificultad. Sin embargo la clasificaci\'on manual tiene sus l\'imites de escalabilidad. 
	
	Como alternativa podr\'ia pensarse el uso de \emph{reglas} para determinar autom\'aticamente si un texto pertenece o no a una clase determinada de documentos.
	
	Ilustremos un ejemplo de regla aplicada autom\'aticamente. Supongamos que un usuario necesita hacer una consulta en una p\'agina de noticias, por ejemplo, necesita tener actualidad sobre noticias relacionadas con las finanzas para tomar decisiones en su negocio. Al usuario entonces le podr\'ia interesar que de hacer una consulta en el sistema dicha consulta se mantenga ejecutando y le provea peri\'odicamente las noticias relativas a finanzas. 
	
	A este tipo de consultas se le denomina consultas permanentes  (o \emph{standing querys} en ingl\'es). Una consulta permanente es aquella que se ejecuta peri\'odicamente en una colecci\'on a la cual nuevos  documentos se adicionan en el tiempo. Toda consulta permanente se puede ver como un tipo de regla que se aplica a un sistema de clasificaci\'on que divide una colecci\'on en documentos que satisfacen la query y documentos que no.
	
	
%	 Por ejemplo las consultas permanentes son un ejemplo de regla aplicada autom\'aticamente. Una consulta permanente es como una consulta normal pero que es ejecutada repetidamente sobre una colecci\'on de textos a la cual se van adicionando documentos nuevos constantemente. Su finalidad es determinar si los nuevos textos pertenecen o no a la clase en cuesti\'on.
	
	Una regla captura una cierta combinaci\'on de palabras claves que identifican una clase. Reglas codificadas a mano pueden llegar a ser altamente escalable, pero crearlas y mantenerlas requiere un elevado costo en recursos humanos.
	
	Existe, sin embargo, un enfoque adicional a los dos anteriores mencionados. Nos referimos al uso de \emph{Aprendizaje de M\'aquinas}. En este enfoque el conjunto de reglas de clasificaci\'on, o en general, el criterio usado para clasificar, es aprendido de forma autom\'atica a partir de los datos de entrenamiento.
	
	Al uso del Aprendizaje de M\'aquinas en la clasificaci\'on de textos se le conoce como clasificaci\'on estad\'istica de texto (o en ingl\'es \emph{statistical text classification}) si el m\'etodo de aprendizaje usado es estad\'itico.
	
	Introduciremos a continuaci\'on la definici\'on forma del problema de clasificaci\'on de textos, en el contexto del Aprendizaje de M\'aquinas.
	
	\begin{definition} \label{Problema_de_clasificacion}
		Sea $\mathcal{{X}}$ el espacio de documentos y $\mathcal{C} := \{c_i \mid c_i \subset \mathcal{X}, i \in \{ 1,2,\dots,n\} \}$ un conjunto fijo de clases (tambi\'en llamadas categor\'ias o etiquetas). Sea adem\'as $D$ un conjunto entrenado de documentos clasificados $(d,c) \in \mathcal X \times \mathcal{C}$. El \emph{problema de la clasificaci\'on de textos} consiste en encontrar, usando m\'etodos o algoritmos de aprendizaje, una funci\'on \emph{clasificadora} $\gamma : \mathcal{X} \rightarrow \mathcal{C}$, que mapee documentos a clases, que satisfaga que $D \subset \gamma$. 	

	\end{definition}
	
	El aprendizaje que toma parte en la b\'usqueda de $\gamma$ es llamado \emph{aprendizaje supervisado} debido a que se necesita la ayuda de uno o varios expertos que creen el conjunto de entrenamiento $D$. Estos expertos son  tambi\'en quienes determinan el conjunto de clases en que se clasificar\'an los textos. Denotaremos el m\'etodo de aprendizaje supervisado descrito por $\Gamma$, el cual act\'ua como una funci\'on que mapea un conjunto de datos de entrenamiento en una funci\'on clasificadora, osea que $\Gamma(D) = \gamma$.
	
	La definici\'on dada en \ref{Problema_de_clasificacion} implica que cada documento pertenece a una sola clase. Pero existe otro tipo de problemas que permiten que un documento pertenezca a m\'as de una clase. Por ahora enfocaremos nuestra atenci\'on en el tipo de una clase.
	


	\subsection{Naive Bayes}

		Uno de los m\'etodos m\'as comunes de aprendizaje supervisado es el conocido como \emph{Naive Bayes} (NB). Este es un m\'etodo de aprendizaje probabil\'istico. La probabilidad de un documento $d$ de pertenecer a una clase $c$ se puede expresar como $P(c\mid d)$. La tarea del algoritmo es encontrar la mejor clase para cada documento $d$. Para ello NB establece que la clase m\'as apropiada para un documento es la m\'as probable, o sea
		
		\[
		c_{map} = \argmax_{c\in\mathcal{C}} P(c \mid d).
		\]
		
		La clase escogida para $d$ se denota por $c_{map}$ debido a que este m\'etodo de clasificaci\'on, de acuerdo a la clase m\'as probable para un documento dado, es conocido como \emph{maximum a posteriori} (MAP).
		
		Sin embargo la probabilidad condicional $P(c \mid d)$ es dif\'icil de determinar. Haciendo uso del \emph{Teorema de Bayes} la probabilidad anterior puede ser expresada como
		
		\[
		P(c \mid d) =\frac{ P(d\mid c) P(c)}{P(d)}.
		\]
		
		El factor de normalizaci\'on $P(d)$ es usualmente ignorado ya que no aporta informaci\'on a la hora de buscar la clase m\'as apropiada para un documento $d$, ya que este tiene el mismo efecto en todos los candidatos. Este c\'alculo puede ser simplificado si lo expresamos en t\'erminos de los t\'erminos en los documentos. Supongamos que $\{t_1, t_2, \dots , t_n \}$ son los t\'erminos que aparecen en $d$. Entonces tenemos que  
		
		\[
			c_{map} = \argmax_{c\in\mathcal{C}} P(c) P(d \mid c) = \argmax_{c\in\mathcal{C}} P(c) \prod_{1\leq k\leq n} P(t_k \mid c),
		\]
		donde $P(t_k \mid c)$ es la probabilidad de que el t\'ermino $t_k$ aparezca en un documento de la clase $c$. Podemos considerar $p(t_k \mid c)$ como una medida de qu\'e tanto demuestra el t\'ermino $t_k$ que $c$ es la clase correcta. El t\'ermino $P(c)$ es conocido como probabilidad previa (\emph{prior probability}) y en caso de que la informaci\'on aportada por los t\'erminos no sea determinante en la selecci\'on podemos siempre escoger la clase con mayor valor de $P(c)$.
		
		Para simplificar a\'un mas el c\'omputo podemos sustituir los valores anteriores por sus logaritmos. Esto reducir\'a el costo de hacer los c\'alculos y adem\'as los errores aritm\'eticos, dado que la multiplicaci\'on se transforma en suma. La clase seleccionada ser\'ia entonces
		
		\[
				c_{map} = \argmax_{c\in\mathcal{C}} \left( \log(P(c))  + \sum_{1\leq k\leq n} \log(P(t_k\mid c)) \right).
		\]
		
		Solo nos queda ver como estimamos los par\'ametros $P(c)$ y $P (t_k\mid c)$, dado que los valores reales no son posibles de calcular.	Para la probabilidad previa podemos contar la frecuencia relativa de cada clase en $D$:
		
		\[P(c) = \frac{ N_c }{N} , \]
		donde $N_c$ es el n\'umero de documentos en la clase $c$ y $N$ es el numero total de documentos. Procedemos de manera similar para la probabilidad espec\'ifica de una palabra en una clase
		\[
			P(t_k\mid c) =\frac{ T_{c,t_k}}{T_{c}},
		\]
		donde $T_{c,t_k}$ indica la cantidad de veces que ocurre  la palabra $t_k$ en todos los documentos de la clase $c$ y $T_{c}$ es la cantidad total de palabras contando repeticiones) en toda la clase $c$. Si embargo, a\'un tenemos un problema con estas f\'ormulas y es que estamos asignando probabilidad cero a todos las clases que no contengan a todas las palabras del documento a clasificar. Para evitar esto adicionamos por defecto una unidad a cada contador lo cual es conocido como \emph{Laplace smoothing}
		\[
			P(t\mid c) = \frac{T_{c,t} + 1}{T_c + |V|},	
		\]
		donde $|V|$ es el n\'umero total de t\'eminos en el vocabulario.
		
		 Es importante destacar que en este m\'etodo estamos obviando la posici\'on de las palabras. Presentamos aqu\'i los algoritmos para entrenar y clasificar usando BN que fueron casi textualmente copiados de \cite[Figure $13.2$]{B1}.
		 
		 
		 
		 \begin{algorithm}{}\label{Alg1}
		 			\caption{TrainMultinomial}
		 	\begin{algorithmic}[1]
		 		
		 		% ENTRADA / SALIDA
		 		\Require{Set of classes $\mathcal{C}$ and training set $D$.} 
%		 		\Ensure{Trained.}
		 		\State{$ V \leftarrow ExtractVocabulary(D)$}
		 		\State{$ N \leftarrow CountDocs(D)$}
		 		\For{$c \in \mathcal{C}$}
		 		\State $N_c \leftarrow CountDocsInClass(D,c)$
		 		\State $prior[c] \leftarrow N_c/N$
		 		\State $text_c \leftarrow ConcatenateTextOfAllDocsInClass(D, c)$
		 		\For{$t \in V$}
		 		\State{$T_{ct} \leftarrow CountTokensOfTerm(text_c, t)$}
		 		\EndFor
		 		\For{$t \in V$}
		 		\State{$condprob[t][c] \leftarrow \frac{T_{c,t} + 1}{\sum_{t'} (T_{c,t'} + 1)}$}
		 		\EndFor 
		 		\EndFor
		 		\State \textbf{\Return} $V, prior, condprob$
		 	\end{algorithmic}
		 	
		 	\smallskip
		 	\tiny{Algoritmo para entrenar Naive Bayes tomado de \cite[Figura 13.2]{B1}}
		 \end{algorithm}
		 
		 \begin{algorithm}\label{Alg2}
		 	\caption{ApplyMultinomialNB}
		 	\begin{algorithmic}[1]
		 		
		 		% ENTRADA / SALIDA
		 		\Require{$\mathcal{C}, V$, $prior$, $condprob$, $d$} 
		 		%		 			\Ensure{Trained.}
		 		\State{$ W \leftarrow ExtractTokensFromDoc(V, d)$}
		 		\For{$c \in \mathcal{C}$}
		 		\State{$ score[c] \leftarrow \log prior[c]$}
		 		\For{$t \in W$}
		 		\State{$ score[c] +=  \log condprob[t][c]$}
		 		\EndFor
		 		\EndFor
		 		\State \textbf{\Return} $\argmax_{c\in \mathcal{C}}(score[c])$
		 	\end{algorithmic}
		 	
		 	\smallskip
		 	\tiny{Algoritmo para aplicar Naive Bayes tomado de \cite[Figura 13.2]{B1}}
		 \end{algorithm}
		 
		Podemos deducir de los algoritmos que la complejidad de ambos es linear en el tiempo que toma escanear la informaci\'on. Dado que esto hay que hacerlo al menos una vez, se puede decir que este m\'etodo tiene complejidad temporal \'optima. Dicha eficiencia hace que NB sea un m\'etodo de clasificaci\'on tan usado.
	
	
	\subsection{Feature Selection}
		Un t\'ermino con ruido (\emph{noise feature}) es aquel que al pertenecer a la representaci\'on de los documentos, provoca un aumento del error de clasificaci\'on de los datos. Por ejemplo, supongamos que tenemos una palabra que ocurre rara vez, pero que en el conjunto de entrenamiento ocurre siempre en la misma clase $c$. Entonces al clasificar un documento nuevo que contiene esta palabra, la misma provocar\'a que el clasificador se incline en cierta medida por seleccionar esta a $c$ como respuesta. Sin embargo, dado que la ocurrencia de esta palabra \'unicamente en $c$ es accidental, claramente no aporta informaci\'on suficiente para la clasificaci\'on y por tanto, al considerar lo contrario, aumenta el error.
		
		Este es uno de los prop\'positos que tiene la selecci\'on de t\'erminos (\emph{feature selection} (FS)). Esta consiste en reducir el vocabulario, considerado en la clasificaci\'on de textos solo un subconjunto del que aparece en el conjunto de entrenamiento. N\'otese que al disminuir el tama\~no del vocabulario aumenta la eficiencia de los m\'etodos de entrenamiento y clasificaci\'on (aunque no es el caso de NB).
		
		Selecci\'on de t\'erminos prefiere un clasificador m\'as simple antes que uno m\'as complejo. Esto es \'util cuando el conjunto de entrenamiento no es muy grande.
		
%		Nos concentraremos en describir la Selecci\'on de t\'erminos para la clasificaci\'on de dos clases.

		 En FS usualmente fijamos una cantidad $k$ de vocablos por cada clase $c$, que ser\'an los usados por el clasificador. Para seleccionar los $k$ t\'erminos deseados establecemos un ranking entre los t\'erminos de la clase, haciendo uso de una funci\'on de medida de utilidad $A(t,c)$, y nos quedamos con los $k$ mejor posicionados. El algoritmo b\'asico consiste en para cada clase $c$ iterar por todos los t\'erminos del vocabulario y computar su medida de utilidad para la clase; para finalmente ordenar los resultados y devolver una lista con los $k$ mejores.
		 
		 Presentaremos a continuaci\'on tres de los m\'etodos de calcular $A(t,c)$ m\'as comunes.
		
		\begin{itemize}
			\item\textbf{Informaci\'on Manual.}
			\smallskip
			
				Computar $A(t,c)$ como el valor esperado de informaci\'on mutua (\emph{Mutual Information} (MI)), nos da una medida de cu\'anta informaci\'on aporta, la presencia en $c$ de un t\'ermino dado, a tomar la decisi\'on correcta de clasificaci\'on de un documento. La siguiente definici\'on fue tomada de \cite[Ecuaci\'on 13.16]{B1}
				\[
					I(U_t;C_t) = \sum_{e_t \in \{ 1,0 \} } \sum_{e_c \in \{ 1,0 \} } P( U_t = e_t, C_t = e_c) \log_2 \frac{P (U_t = e_t, C_t = e_c) }{ P(U_t = e_t) P(C_t = e_c) },
				\]
				donde $U_t$ es una variable aleatoria que toma valor $e_t = 1$ si el documento contiene el t\'ermino $t$ y $e_t = 0$ en otro caso, y $C$ es otra variable aleatoria que toma valor $e_c = 1$ si el documento est\'a en la clase $c$ y $e_c = 0  $ en otro caso. 
				
				\smallskip
	
				MI mide cu\'anta informaci\'on un t\'ermino contiene acerca de una clase. Por tanto, mantener los t\'erminos que est\'an cargados de informaci\'on, y eliminar los que no, contribuye a reducir el ruido y mejorar la precisi\'on del clasificador.
	
			\smallskip
			\item\textbf{Selecci\'on Chi cuadrado $\chi^2$.} 
			\smallskip
			
			En estad\'istica se dice que dos eventos son independientes si el resultado de uno no afecta al resultado del otro. Esto se puede escribir formalmente como $P(AB) = P(A) P(B)$. En estad\'istica el test $\chi^2$ se usa para medir el grado de independencia de dos eventos. En FS podemos entonces considerar aplicar este test asumiendo como eventos la ocurrencia de los t\'erminos y la ocurrencia de las clases.  La siguiente definici\'on fue tomada de \cite[Ecuaci\'on 13.18]{B1}
			\[
				\chi^2(D,t,c) = \sum_{e_t\in \{ 1, 0 \}} \sum_{e_c\in \{ 1, 0 \}} \frac{(N_{e_te_c} - E_{e_t e_c}) ^2 } { E_{e_t e_c}},
			\]
			donde $N$ es la frecuencia seg\'un $D$, $E$ es la frecuencia esperada y $e_t$ y  $e_c$ se definen como en la medida anterior.
			
			\smallskip
			\item \textbf{Selecci\'on basada en frecuencia}.
			\smallskip
			
			 Esta medida consiste en priorizar los t\'erminos que son m\'as comunes en la  clase. Puede ser calculada de dos formas diferentes. La primera es cantidad de repeticiones de un t\'ermino en los documentos de una clase, conocida como frecuencia en colecci\'on. La otra es frecuencia de documentos, y se calcula como la cantidad de documentos en la clase que contienen al t\'ermino en cuesti\'on.
			
			\smallskip
				
			Cuando son seleccionados varios miles de t\'erminos, entonces esta medida es bastante buena. Esta es preferible a otros m\'etodos m\'as complejos cuando se aceptan soluciones sub\'optimas.
			
		\end{itemize}
	
	\subsection{K Nearest Neighbor}
	
		En el algoritmo de Naive Bayes represent\'abamos los documentos como vectores booleanos de t\'erminos. Luego vimos que hay t\'erminos que no eran relevantes y que aportaban ruido, y lo solucionamos seleccionando para el clasificador solamente un subconjunto de todos los t\'erminos. A\'un as\'i estamos clasificando la relevancia de cada t\'ermino de manera binaria en relevante o no relevante (que aporta ruido).
		
		 El m\'etodo que presentamos en esta secci\'on, as\'i como otros similares, asignan a cada t\'ermino cierto valor de importancia relativa al documento en que aparece. Para esto se cambia la representaci\'on de los documentos a vectores de $\mathbb{R}^{|V|}$, donde a cada componente corresponde cierto peso que se le asigna al t\'ermino correspondiente a esta. Entonces, el espacio de documentos $\mathcal{X}$ (dominio de $\gamma$) es $\mathbb{R}^{|V|}$. A esta forma de representaci\'on de documentos se le conoce como modelo de espacio de vectores. La hip\'otesis b\'asica para usar el modelo de espacio de vectores la presentamos a acontinuaci\'on y fue tomadada de \cite[p. 289]{B1}.
		 
		 \textbf{Hip\'otesis de contig\"uidad:} Documentos en la misma clase forman una regi\'on contigua  y regiones de diferentes clases no se superponen.
		 
		 Las decisiones de muchos clasificadores basados en espacio de vectores dependen de una noci\'on de distancia. Pueden ser usadas por ejemplo similitud basado en el coseno (del \'angulo formado entre los vectores) o distancia Euclideana. Por lo general no hay mucha diferencia entre usar una u otra de estas distancias.
		 
		 La tarea de la clasificaci\'on en el modelo de espacio de vectores es determinar las fronteras entre los documentos pertenecientes a una u otra clase. Estas \'ultimas son llamadas fronteras de decisi\'on ya que dividen el espacio en diferentes poliedros, tales que si un documento pertenece a uno determinado, autom\'aticamente sabemos de qu\'e clase es. 
		 
		 En K Nearest Neighbor la frontera de decisi\'on se determina localmente. En este asignamos cada documento a la misma clase que la mayor\'ia de los $k$ puntos m\'as cercanos al documento. Basado en la hip\'otsis de contig\"uidad esperamos que el documento $d$ pertenezca a la misma clase que aquellos m\'as cercanos a \'el.
		 
		En $kNN$ para subdividir el espacio de documentos en regiones, dado un $k \in \mathbb{N}$ fijo, consideramos cada regi\'on como el conjunto de puntos para los cuales los $k$ puntos m\'as cercanos son los mismos. Estas regiones son poliedros convexos. Luego para cada una de estas regiones existe una clase a la que pertenecen todos sus puntos, que es aquella a la que pertenecen la mayor\'ia de los documentos, ya clasificados, que est\'an dentro de la regi\'on. En caso de que haya empate, la decisi\'on de a que clase asignar a un nuevo documento que pertenece a esta regi\'on del espacio, es tomada aleatoriamente entre las clases empatadas.
		
		El par\'ametro $k$ es usualmente seleccionado basado en el conocimiento que se posee sobre los problemas de clasificaci\'on similares al que se tiene. Otra forma de seleccionar $k$ es usando conjuntos de documentos de prueba (ya clasificados) para ver que valor de $k$ producen mejores resultados.
		
		Tambi\'en hay variantes del m\'etodo donde lo que se hace es calcular una similitud entre el documento $d$ a clasificar y cada uno de los $k$ m\'as cercanos, usando, por ejemplo, el coseno entre los vectores. Luego se hace un ranking entre las clases a las que pertenecen cada uno de los $k$ puntos. Para ello se calcula para cada $c$, la suma de la similitud entre $d$ y cada uno de los puntos que pertenecen a $c$ y a la vez est\'an entre los $k$ mencionados. La siguiente funci\'on $score$ produce el resultado deseado
		\[
			score(c,d)  = \sum_{d'\in S_k(d)} I_c(d') \cos(\overrightarrow{v}(d'),\overrightarrow{v}(d)),
		\]
		donde $S_k(d)$ es el conjunto de los $k$ puntos m\'as cercanos a $d$ e $I_c(d')$ es $1$ o $0$ en dependencia de si $d'$ pertenece a la clase $c$ o no. Finalmente se selecciona para $d$ la clase $c$ que m\'as alto aparezca en el ranking. En ocasiones esta variante presenta mayor exactitud que la anterior.
		
		
		\begin{algorithm}{}
			\caption{Train-kNN}
			\begin{algorithmic}[1]
				
				% ENTRADA / SALIDA
				\Require{Set of classes $\mathcal{C}$ and training set $D$.} 
				%		 		\Ensure{Trained.}
				\State{$ D' \leftarrow PreProcess(D)$}
				\State{$ k \leftarrow Selectk(\mathcal{C}, D')$}
				\State \textbf{\Return} $D', k$
			\end{algorithmic}
			
			\smallskip
			\tiny{Algoritmo para entrenar KNN tomado de \cite[Figura 14.7]{B1}}
		\end{algorithm}
		
		\begin{algorithm}{}
			\caption{Apply-kNN}
			\begin{algorithmic}[1]
				
				% ENTRADA / SALIDA
				\Require{$\mathcal{C}$, $D'$, $k$, $d$}
				%		 		\Ensure{Trained.}
				\State{$ S_k \leftarrow ComputeNearestNeighbors(D',k,d)$}
				\For{$c_j \in \mathcal{C}$}
					\State{$ p_j \leftarrow | S_k \cap c_j | /k$ }
				\EndFor
				\State \textbf{\Return} $\argmax_j p_j$
			\end{algorithmic}
				
			\smallskip
			\tiny{Algoritmo para aplicar KNN tomado de \cite[Figura 14.7]{B1}}
		\end{algorithm}
		
		
	\subsection{Medidas de evaluaci\'on}
			
			Una alta exactitud en la clasificaci\'on de los datos de entrenamiento no necesariamente se traduce en resultados correctos en los nuevos documentos introducidos. Puede sucede que el sistema resulte sobreentrenado (\emph{over-fitting}) o subentrenado (\emph{under-fitting}). El primero de los casos ocurre cuando el modelo se encuentra con muchos datos. En este caso el sistema aprende de los ruidos y de las entradas inexactas lo que provoca que el clasificador aprenda incorrectamente a clasificar los elementos de alguna clase. Por otro lado under-fitting ocurre cuando el sistema tiene informaci\'on muy vaga sobre la frontera entre clases lo que provoca cierta aleatoriedad en la clasificaci\'on.
			
			Para evaluar la labor de un clasificador se debe emplear un conjunto de documentos diferentes al conjunto entrenante el cual se llama conjunto de prueba. Este debe estar en todo momento aislado del conjunto de entrenamiento. Si se juntan el clasificador puede aprender en su entrenamiento a reconocer los documentos del conjunto de prueba, y luego  al evaluar el mismo, el desempe\~no del sistema ser\'a mucho m\'as alto que el resultado real que tenga cuando se ponga en uso. Usualmente los datos que se tienen se dividen entre el conjunto entrenante y de prueba a raz\'on de $4:1$.
			
			Las siguientes tres m\'etricas pueden ser utilizadas para evaluar clasificadores de dos clases (c y \~c):	
					
			\begin{itemize}
				
				\item\textbf{Precisi\'on}
				
				\[
				Precision = \frac{\emph{N\'umero de documentos correctamente identificados como $c$} }{\emph{N\'umero de documentos identificados como $c$}}
				\]
				
				\item\textbf{Recobrado}
				
				\[
				recobrado = \frac{\emph{N\'umero de documentos correctamente identificados como $c$} }{\emph{N\'umero de documentos que pertenecen a $c$}}
				\]
				
				\item\textbf{Medida $F$} 
				
				\[
					F = 2 \times  \frac{Precision \times Recobrado}{Precision + Recobrado}
				\]
				
				
			\end{itemize}
			
			El mismo resultado se puede ver usando la matriz de contingencia. Una matriz de contingencia muestra de todas las pruebas realizadas con el clasificador, cu\'antas resultaron en verdadero positivo, falso positivo, falso negativo y verdadero negativo.
			
			En el caso que la cantidad de clases sea mayor que dos, podemos computar promedios entre las m\'etricas anteriores viendo el clasificador como uno de dos clases aplicado a cada clase $c$ (y su complemento). Existen dos promedios que son los m\'as usados para esto. Macropromedio calcula simplemente la media entre las medidas anteriores para cada clasificador de dos clases. Microaverage primero adiciona las matrices de contingencia correspondientes a diferentes clases y luego aplica una de las m\'etricas anteriores sobre la matriz resultante. 
			
			
	\subsection{Ventajas}
				\begin{itemize}
					\item Al usar el aprendizaje supervisado se puede tener una idea exacta sobre las clases de los documentos, dado que estas se crean basado en caracter\'isticas expl\'icitas de los mismos. 
					\item La salida suele ser m\'as precisa que en el aprendizaje supervisado
					\item El proceso es f\'acil de comprender ya que las acciones de la m\'aquina se reconocen con precisi\'on.
					\item Es m\'as facil de trabajar con el aprendizaje supervisado que con el no supervisado. 
					\item Se eval\'ua y recibe retroalimentaci\'on para comprobar la correctitud. 
				\end{itemize}
		
	\subsection{Desventajas}
				\begin{itemize}
					\item Se necesita un especialista que determine cu\'ales ser\'an las clases en que se desea clasificar.
					\item Se requiere de un conjunto de datos entrenantes.
					\item Por lo general no se resuelven tareas tan complejas como en el aprendizaje supervisado.
					\item Los algoritmos solo funcionan dentro de las restricciones que se le han impuestos por lo que no proporcionan soluciones creativas.
					\item Suelen requerir mucho tiempo de entrenamiento.
					\item Puede predecir la salida incorrecta si los datos de prueba son diferentes de los datos de entrenamiento.
				\end{itemize}
				
	\subsection{Aplicaciones en la Recuperaci\'on de la Informaci\'on.} \label{App_RI}
		
		El aprendizaje de m\'aquinas es ampliamente usado en la actualidad para resolver una variedad de problemas de muchas esferas. La clasificaci\'on, de conjunto con la regresi\'on, son unas de las grandes tareas que tiene el aprendizaje de m\'aquinas supervisado. Algunos problemas interesantes que resuelve la clasificaci\'on se encuentran dentro del marco de la Web y los Sistemas de Recuperaci\'on de la informaci\'on. Anteriormente present\'abamos uno de ellos, que es la recuperaci\'on de consultas permanentes.
		
		La organizaci\'on del correo personal es otros de los ejemplos relacionados con la Web que pueden ser resueltos mediante clasificaci\'on. En muchas ocasiones una persona tiene un n\'umero grande de correos en el buz\'on de entrada. A la hora de revisarlo ser\'ia deseable que pudiera dirigirse directamente a aquellos que son de inter\'es para \'el. Para ello la organizaci\'on autom\'atica del correo en carpetas podr\'ia ser una ventaja invaluable. Un ejemplo particular ser\'ia la carpeta de correo spam.
		
		Otro problema que se pudiera resolver ser\'ia la clasificaci\'on de valoraciones sobre algo en positivas o negativas. Esto tendr\'ia numerosas aplicaciones pr\'acticas como pudiera ser la selecci\'on de una pel\'icula. Un usuario podr\'ia revisar la cantidad de comentarios negativos, antes de lanzarse a disponer de dos horas de su tiempo viendo un material audiovisual que no resultar\'a muy placentero.
		
		El uso m\'as evidente que tiene la clasificaci\'on el la RI es la clasificaci\'on de documentos en t\'opicos. Esta clasificaci\'on facilitar\'ia la implementaci\'on de un motor de b\'usqueda vertical, que permita al usuario restringir su b\'usqueda al tema deseado. Un SRI con semejante caracter\'istica permite hacer una b\'usqueda m\'as precisa en las consultas m\'as especializadas por el usuario.
		
		En RI es muy \'util contar con un \'indice de los contenidos almacenados. En el proceso de creaci\'on del mencionado \'indice la clasificaci\'on puede jugar un importante papel. Por citar algunos ejemplos puede usarse para detectar el lenguaje de un documento, la segmentaci\'on y capitalizaci\'on de las palabras y el codificado del documento.
		

	\subsection{Otros ejemplos de aplicaci\'on}

Ya hablamos en la ep\'igrafe \ref{App_RI} acerca de la importancia de la clasificaci\'on en numerosos problemas de la sociedad. En especial nos centramos en ejemplificar algunos de los problemas espec\'ificos de la Recuperaci\'on de la Informaci\'on en que es aplicable la clasificaci\'on. Trataremos en esta secci\'on de ver la importancia de la clasificaci\'on en problemas m\'as generales de la humanidad.

La bioinform\'atica es una rama cient\'ifica que ha adquirido un desarrollo impresionante en los \'ultimos an\~os. Esta se dedica al desarrollo de herramientas inform\'aticas para aplicarlas en la gesti\'on y an\'alisis de datos biol\'ogicos. Algunas  de las aplicaciones m\'as importantes que tiene la clasificaci\'on, y en general, el aprendizaje autom\'atico, es precisamente en esta rama de la ciencia. Con el uso de sus algoritmos se permite automatizar la b\'usqueda de patrones en conjuntos de datos, que puedan ayudar a entender diferentes procesos biol\'ogicos que se manifiestan en los mismos. Debido a los grandes vol\'umenes de datos que se tienen ha crecido mucho en los \'ultimos a\~nos la aplicaci\'on de la clasificaci\'on en esta disciplina.

Una de estas aplicaciones biol\'ogicas de las que hablabamos es la predicci\'on de gen. Esta consiste en determinar que fracciones de una secuencia de ADN codifican prote\'inas. Tambi\'en se ve aplicado en la investigaci\'on de comunidades de microbios en nichos ecol\'ogicos. As\'i se puede obtener informaci\'on taxon\'omica y metab\'olica de las comunidades estudiadas. Existen otras muchas aplicaciones en esta ciencia entre las que podemos mencionar la prote\'omica, los microarrays y la biolog\'ia de sistemas.

Sali\'endonos un tanto del marco de la bioinform\'atica, pero no del todo lejos de la biolog\'ia, encontramos algunas otras aplicaciones interesantes. Nos referimos al reconocimiento de algunos rasgos distintivos de una persona, como pueden ser, reconocimiento de rostros, huellas o voz. Este \'ultimo lo tenemos al alcance de nuestras manos en asistentes virtuales como Siri o  Google. Estas aplicaciones se entrenan para reconocer caracter\'isticas distintivas de tu voz, mediante un proceso de aprendizaje supervisado, para luego poder diferencias c\'uando eres t\'u quien les est\'a hablando.

El mercado financiero es una de los espacios de la vida del hombre donde m\'as repercute la clasificaci\'on. Se puede emplear en mejorar el mercado digital, las ventas en l\'inea, identificar el valor de vida \'util de un cliente, la tasa de abandono, recomendaciones de productos y an\'alisis de impacto de campa\~nas de mercado.

En la seguridad inform\'atica tambi\'en se evidencia la clasificaci\'on en acciones como detecci\'on de virus, enlaces maliciosos y fraude. Otras aplicaciones en que puede verse tambi\'en son el Internet de las cosas (IoT), los motores de recomendaci\'on y la fijaci\'on de precios din\'amicos de productos.

\section{Conclusiones}

	La metodolog\'ia usada en la clasificaci\'on y el agrupamiento es diferente y por ende el resultado esperado de sus algoritmos difiere tambi\'en entre s\'i. Sin embargo, evidencian la creciente importancia que tiene la computaci\'on y en especial el aprendizaje de m\'aquina, en ambas variantes, supervisado y no supervisado, para ayudar a la humanidad en tareas esenciales que se evidenciaban en el mundo incluso antes de su existencia. Hemos analizado aqu\'i algunos de los algoritmos m\'as importantes de ambas metodolog\'ias. 
	
	Aunque el objetivo de la clasificaci\'on y agrupamiento sea similar, hemos visto que esencialmente se utilizan en diferentes situaciones.  Sus aplicaciones a la resoluci\'on de problemas se ven a menudo reflejados en la Recuperaci\'on de la Informaci\'on y en la Web. Aunque,  antes de aplicarlas en alg\'un problema, es siempre bueno estar conscientes de sus ventajas y desventajas, las cuales en ocasiones son opuestas.
	
	
	

	

\begin{thebibliography}{20}
	\bibitem{B1} Maning C. D.: \emph{An Introduction To Information Retrieval} (2009).
	\bibitem{B2} Berlin Chen: \emph{Clustering Techniques for
		Information Retrieval} (2015).
	\bibitem{B3} Edie Rasmussen: \emph{Information Retrieval} (2021).
\end{thebibliography}

\end{document}


