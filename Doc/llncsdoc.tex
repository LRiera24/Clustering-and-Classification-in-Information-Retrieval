% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{color}

\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage[noend]{algpseudocode}


\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[spanish]{babel}

% Keywords command
\providecommand{\keywords}[1]
{
	\small	
	\textbf{\textit{Palabras claves:}} #1
}

%
\begin{document}
\markboth{Sistemas de Recuperaci\'on de Informaci\'on}{Aplicaciones del agrupamiento y de la clasificación en la RI en la Web}
\thispagestyle{empty}
\begin{flushleft}
\LARGE\bfseries Sistemas de Recuperaci\'on de Informaci\'on\\[2cm]
\end{flushleft}
\rule{\textwidth}{1pt}
\vspace{2pt}
\begin{flushright}
\Huge
\begin{tabular}{@{}l}
Aplicaciones del\\
agrupamiento y\\ 
de la clasificación\\ 
en la recuperaci\'on\\ 
de informaci\'on\\
en la Web\\[6pt]
\end{tabular}
\end{flushright}
\rule{\textwidth}{1pt}
\vfill
\begin{flushleft}
\large\itshape
\begin{tabular}{@{}l}
{\large\upshape\bfseries Autores}\\[8pt]
Laura Victoria Riera P\'erez\\[5pt]
Marcos Manuel Tirador del Riego
\end{tabular}
\end{flushleft}

\newpage
\pagenumbering{gobble}
\begin{abstract}
	Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum pretium libero non odio tincidunt semper. Vivamus sollicitudin egestas mattis. Sed vitae risus vel ex tincidunt molestie nec vel leo. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Maecenas quis massa tincidunt, faucibus magna non, fringilla sapien. In ullamcorper justo a scelerisque egestas. Ut maximus, elit a rutrum viverra, lectus sapien varius est, vel tempor neque mi et augue. Fusce ornare venenatis nunc nec feugiat. Proin a enim mauris. Mauris dignissim vulputate erat, vitae cursus risus elementum at. Cras luctus pharetra congue. Aliquam id est dictum, finibus ligula sed, tempus arcu. 
	
	\vspace{1em}
	\keywords{one, two, three, four}
\end{abstract} 
\thispagestyle{empty}


\newpage
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}

\newpage
\pagenumbering{arabic}

\section{Introducci\'on}

\section{Agrupamiento}

Los algoritmos de agrupamiento, como su nombre lo indica, agrupan un conjunto de documentos en subconjuntos o clústeres. Son utilizados para generar una estructura de categorías que se ajuste a un conjunto de observaciones. 

Los grupos formados deben tener un alto grado de asociación entre los documentos de un mismo grupo, es decir, deben ser lo m\'as similares posibles, y un bajo grado entre miembros de diferentes grupos. 

Es la forma más común de \textit{aprendizaje no supervisado}, es decir no hay ningún experto humano que haya asignado documentos a clases. Es la distribución y composición de los datos lo que determinará la pertenencia al clúster. 

Si bien el agrupamiento a veces es denominada clasificación automática, esto no es estrictamente exacto ya que las clases formadas no se conocen antes de tratamiento, como implica la clasificación, sino que se definen por los elementos que se les asignan.

La entrada clave para un algoritmo de agrupamiento es la medida de distancia. Diferentes medidas de distancia dan lugar a diferentes agrupamientos. En el agrupamiento de documentos, la medida de distancia es a menudo también la distancia euclidiana. 

\vspace{1em}
\textbf{Hip\'otesis de agrupamiento:} \textit{Los documentos en el mismo grupo se comportan de manera similar con respecto a la relevancia para las necesidades de información.}
	
\vspace{0.3em}
La hipótesis establece que si hay un documento de un grupo que es relevante a una solicitud de búsqueda, entonces es probable que otros documentos del mismo clúster también sean relevantes. 

\vspace{1em}
Los algoritmos de agrupamiento pueden ser clasificados seg\'un la pertenencia a los grupos en \textit{agrupamiento exclusivo o fuerte} (hard clustering) donde cada documento es miembro de exactamente un grupo; o \textit{agrupamiento difuso o suave} (soft clustering) donde un documento tiene membresía fraccionaria en varios grupos.

Tambi\'en pueden clasificarse seg\'un el tipo de estructura impuesta sobre los datos como \textit{agrupamiento particionado o plano} (flat clustering) o \textit{agrupamiento jer\'arquico} (hierarchical clustering).

\subsection{Agrupamiento particionado}

El \textit{agrupamiento particionado} crea un conjunto  de clústeres sin ninguna estructura explícita que los relacione entre sí. 

\subsubsection{Medida de similitud}

\subsubsection{Medidas de evaluaci\'on}


\begin{itemize}
	\item \textbf{Pureza:} Para calcular la pureza, cada grupo se asigna a la clase que es más frecuente en el grupo, y luego se mide la precisión de esta asignación contando el número de documentos correctamente asignados y dividiendo por N.
	
	Formalmente
	
	\begin{center}
		$purity(\Omega, \mathbb{C}) = \dfrac{1}{N} \sum_{k}max_{j} |\omega_{k} \cap c_{j}| $
	\end{center}
	donde $ \Omega = \{\omega_{1}, \omega_{2}, ... , \omega_{K}\} $ es el conjunto de cl\'usters y $ C = \{c_{1}, c_{2}, ... , c_{J}\} $ es el conjunto de clases. Se interpreta $ \omega_{k} $ como el conjunto de documentos en el cl\'uster y $ c_{j} $ como el conjunto de documentos que pertenecen a esa clase.
	
	Malos agrupamientos tienen valores de pureza cercanos a 0, mientras que un agrupamiento perfecto tiene una pureza 1.
	
	La alta pureza es fácil de lograr cuando el número de grupos es grande, en particular, la pureza es 1 si cada documento tiene su propio grupo. Sin embargo en este caso tendr\'iamos la mayor cantidad de grupos posibles, por lo que nuestra soluci\'on no tendr\'ia calidad.
	\vspace{1em}
	\item \textbf{Información mutua normalizada:} Una medida que nos permite hacer la compensaci\'on entre calidad y pureza es la información mutua normalizada o $ NMI $ por sus siglas en ingl\'es.
	
\begin{center}
		$ NMI(\Omega, \mathbb{C}) = \dfrac{I(\Omega, \mathbb{C})}{[H(\Omega) + H(\mathbb{C})]/2}
	 $
\end{center}
	
	I es la informaci\'on mutua:
	\begin{center}
		$ I(\Omega, \mathbb{C}) = \sum_{k}\sum_{j}P(\omega_{k} \cap c_{j})\log\frac{P(\omega_{k} \cap c_{j})}{P(\omega_{k})P(c_{j})} $
	\end{center}
	donde $ P(\omega_{k}) $, $ P(c_{j}) $ y $ P(\omega_{k} \cap c_{j}) $ son las probabilidades de que un documento sea del cl\'uster $ \omega_{k} $, clase $ c_{j} $, y est\'e la intersección de $ \omega_{k} $ y $ c_{j} $, respectivamente.
	
	\vspace{0.5em}
	Para estimaciones de máxima verosimilitud
	de las probabilidades (es decir, la estimación de cada probabilidad es la correspondiente frecuencia relativa):
	\begin{center}
		$ I(\Omega, \mathbb{C}) = \sum_{k}\sum_{j}|\omega_{k} \cap c_{j}|\log\frac{N|\omega_{k} \cap c_{j}|}{|\omega_{k}||c_{j}|} $
	\end{center}
	
	H es la entropía tal como se define en el Capítulo 5 (página 99):
	La característica de una distribución de probabilidad discreta3 P que determina
	sus propiedades de codificación (incluyendo si un código es óptimo)
	
	\begin{center}
		$ H(\Omega) = -\sum_{k}P(\omega_{k} \log P(\omega_{k} $
	\end{center}


	donde, de nuevo, la segunda ecuación se basa en estimaciones de máxima verosimilitud
	de las probabilidades
	
	$ I(\Omega, \mathbb{C})$ mide la cantidad de información por la cual nuestro conocimiento sobre las clases aumenta cuando se nos dice cu\'ales son los grupos. 
	
	El mínimo de $ I(\Omega, \mathbb{C})$ es 0 si el agrupamiento es aleatorio con respecto a pertenencia a clases. En ese caso, sabiendo que un documento está en un determinado
	cluster no nos da ninguna información nueva sobre cuál podría ser su clase.
	
	Se alcanza la máxima información mutua para un agrupamiento exacto que perfectamente
	recrea las clases, pero también si los grupos en exact se subdividen en conglomerados más pequeños. En particular, un agrupamiento con K = N los grupos de documentos tienen MI máximo. Entonces MI tiene el mismo problema que la pureza: no penaliza cardinalidades grandes y por lo tanto no formaliza nuestro sesgo
	que, en igualdad de condiciones, menos grupos son mejores.
	
	La normalización por el denominador $ [H(\Omega) + H(\mathbb{C})]/2 $ soluciona este problema ya que la entropía tiende a aumentar con el número de clusters. Por ejemplo, $ H(\Omega) $ alcanza su log N máximo para K = N, lo que asegura que NMI es bajo para K = N. Debido a que NMI está normalizado, podemos usar para comparar agrupaciones con diferentes números de agrupaciones. Lo particular
	Se elige la forma del denominador porque $ [H(\Omega) + H(\mathbb{C})]/2 $ es una parte superior estrecha atado en $ I(\Omega, \mathbb{C})$. Por lo tanto, NMI es siempre un número entre 0 y 1.
	
	\vspace{1em}
	\item \textbf{\'Indice de frontera (Rand Index)}: Una alternativa a esta interpretación teórica de la información del agrupamiento es
	para verlo como una serie de decisiones, una para cada uno de los $ N(N - 1)/2 $ pares de
	documentos de la colección. Queremos asignar dos documentos al mismo
	clúster si y sólo si son similares. Una decisión positiva verdadera (TP) asigna
	dos documentos similares al mismo grupo, una verdadera decisión negativa (TN) as-
	firma dos documentos diferentes a diferentes grupos. Hay dos tipos
	de errores que podemos cometer. Una decisión de falso positivo (FP) asigna dos disim-
	documentos ilar al mismo clúster. Una decisión de falso negativo (FN) asigna
	dos documentos similares a diferentes agrupaciones. El índice de Rand (IR) mide
	ÍNDICE ALEATORIO
	Rhode Island
	el porcentaje de decisiones que son correctas. Es decir, es simplemente precisión.
	
	\begin{center}
		$ RI = \dfrac{TP + TN}{TP + FP + FN + TN} $
	\end{center}

	El índice de Rand otorga el mismo peso a los falsos positivos y falsos negativos.
	Separar documentos similares a veces es peor que poner pares de dis-
	documentos similares en el mismo grupo.
	
	\vspace{1em}
	\item \textbf{Medida F:} Podemos usar la medida F, vista anteriormente en conferencia, para penalizar los falsos negativos más fuertemente que los falsos positivos seleccionando un valor $ \beta > 1 $, dando así más peso a la recuperación.
	\begin{center}
	$ P = \dfrac{TP}{TP + FP}  $ \hspace{2em}
	$ R = \dfrac{TP}{TP + FN} $
	
	\vspace{1em}
	$ F_{\beta} = \dfrac{(\beta^{2} + 1)PR}{\beta^{2}P + R} $
	\end{center}
\end{itemize}

\subsubsection{Algoritmos}
\begin{itemize}
	\item K-means
\end{itemize}


\subsection{Agrupamiento jer\'arquico}

\begin{itemize}
\item Hierarchical agglomerative clustering

\item Medidas de similitud:
\begin{itemize}
	\item Single link clustering
	\item Complete link clustering
	\item Centroid clustering
\end{itemize}

\item Evaluaci\'on de calidad:
\begin{itemize}
\item Group average link
\item M\'etodo de Ward
\end{itemize}

\item Divisive clustering

\item Cluster labeling

\item Algoritmos:
\begin{itemize}
	\item Algoritmo HAC
	\item Divisive Clustering
\end{itemize}
\end{itemize}

\subsection{Aplicaciones a la RI} 
\begin{itemize}
	\item Search result clustering
	
	\item Scatter-Gather
	
	\item Collection clustering
	
	\item Language modeling
	
	\item Cluster-based retrieval
\end{itemize}

\subsection{Ventajas} 

\subsection{Desventajas} 

\section{Clasificaci\'on}

	El problema de clasificaci\'on en sentido general consiste en determinar dentro un conjunto de clases a cu\'a de ellas pertenece un objeto dado. En el marco de este documento estamos interesados en estudiar la clasificaci\'on de textos. 
	
	La forma m\'as simple de clasificaci\'on de un conjunto de textos es la denominada clasificaci\'on en dos clases. Dichas clases est\'an determinadas por un t\'opico espec\'ifico y ser\'ian: \emph{documentos sobre dicho tema } y \emph{documentos no relacionados con el tema}. Este tipo de clasificaci\'on en ocasiones es llamada \emph{filtrado}. 
	
	La forma m\'as antigua de llevar a cabo la clasificaci\'on es manualmente. Por ejemplo, los bibliotecarios clasifican los libros de acuerdo a ciertos criterios, de modo que encontrar una informaci\'on buscada no resulte una tarea de gigante dificultad. Sin embargo la clasificaci\'on manual tiene sus l\'imites de escalabilidad. 
	
	Como alternativa podr\'ia pensarse el uso de \emph{reglas} para determinar autom\'aticamente si un texto pertenece o no a una colecci\'on de documentos relacionados con un tema. Por ejemplo las consultas permanentes son un ejemplo de regla aplicada autom\'aticamente. Una consulta permanente es como una consulta normal pero que es ejecutada repetidamente sobre una colecci\'on de textos a la cual se van adicionando documentos nuevos constantemente. Su finalidad es determinar si los nuevos textos pertenecen o no a la clase en cuesti\'on.
	
	Una regla captura una cierta combinaci\'on de palabras claves que identifican una clase. Reglas codificadas a mano pueden llegar a ser altamente escalable, pero crearlas y mantenerlas requiere un elevado costo en recursos humanos.
	
	Existe, sin embargo, un enfoque adicional a los dos anteriores mencionados. Nos referimos al uso de \emph{Aprendizaje de M\'aquinas}. En este enfoque el conjunto de reglas de clasificaci\'on, o en general, el criterio usado para clasificar, es aprendido de forma autom\'atica a partir de los datos de entrenamiento.
	
	Introduciremos a continuaci\'on la definici\'on forma del problema de clasificaci\'on de textos, en el contexto del Aprendizaje de M\'aquinas.
	
	\begin{definition} \label{Problema_de_clasificacion}
		Sea $\mathcal{{X}}$ el espacio de documentos y $\mathcal{C} := \{c_i \mid c_i \subset \mathcal{X}, i \in \{ 1,2,\dots,n\} \}$ un conjunto fijo de clases (tambi\'en llamadas categor\'ias o etiquetas). Sea adem\'as $D$ un conjunto entrenado de documentos clasificados $(d,c) \in \mathcal X \times \mathcal{C}$. El \emph{problema de la clasificaci\'on de textos} consiste en encontrar, usando m\'etodos o algoritmos de aprendizaje, una funci\'on \emph{clasificadora} $\gamma : \mathcal{X} \rightarrow \mathcal{C}$, que mapee documentos a clases, que satisfaga que $D \subset \gamma$. 	

	\end{definition}
	
	El aprendizaje que toma parte en la b\'usqueda de $\gamma$ es llamado \emph{aprendizaje supervisado} debido a que se necesita la ayuda de uno o varios expertos que creen el conjunto de entrenamiento $D$. Estos expertos son  tambi\'en quienes determinan el conjunto de clases en que se clasificar\'an los textos. Denotaremos el m\'etodo de aprendizaje supervisado descrito por $\Gamma$, el cual act\'ua como una funci\'on que mapea un conjunto de datos de entrenamiento en una funci\'on clasificadora, osea que $\Gamma(D) = \gamma$.
	
	La definici\'on dada en \ref{Problema_de_clasificacion} implica que cada documento pertenece a una sola clase. Pero existe otro tipo de problemas que permiten que un documento pertenezca a m\'as de una clase. Por ahora enfocaremos nuestra atenci\'on en el tipo de una clase.
	
	\begin{subsection}{Naive Bayes}

		Uno de los m\'etodos m\'as comunes de aprendizaje supervisado es el conocido como \emph{Naive Bayes} (NB). Este es un m\'etodo de aprendizaje probabil\'istico. La probabilidad de un documento $d$ de pertenecer a una clase $c$ se puede expresar como $P(c\mid d)$. La tarea del algoritmo es encontrar la mejor clase para cada documento $d$. Para ello NB establece que la clase m\'as apropiada para un documento es la m\'as probable, o sea
		
		\[
		c_{map} = \argmax_{c\in\mathcal{C}} P(c \mid d).
		\]
		
		La clase escogida para $d$ se denota por $c_{map}$ debido a que este m\'etodo de clasificaci\'on, de acuerdo a la clase m\'as probable para un documento dado, es conocido como \emph{maximum a posteriori} (MAP).
		
		Sin embargo la probabilidad $P(c \mid d)$ es dif\'icil de determinar. Haciendo uso del \emph{Teorema de Bayes} podemos expresarla como
		
		\[
		P(c \mid d) =\frac{ P(d\mid c) P(c)}{P(d)}.
		\]
		
		El factor de normalizaci\'on $P(d)$ es usualmente ignorado ya que no aporta informaci\'on a la hora de buscar la clase m\'as apropiada para un documento $d$, ya que este tiene el mismo efecto en todos los candidatos. Este c\'alculo puede ser simplificado lo expresamos en t\'erminos de los t\'erminos en los documentos. Supongamos que $\{t_1, t_2, \dots , t_n \}$ son los t\'erminos que aparecen en $d$. Entonces tenemos que  
		
		\[
			c_{map} = \argmax_{c\in\mathcal{C}} P(c) P(d \mid c) = \argmax_{c\in\mathcal{C}} P(c) \prod_{1\leq k\leq n} P(t_k \mid c),
		\]
		donde $P(t_k \mid c)$ es la probabilidad de que el t\'ermino $t_k$ aparezca en un documento de la clase $c$. Podemos considerar $p(t_k \mid c)$ como una medida de qu\'e tanto demuestra el t\'ermino $t_k$ que $c$ es la clase correcta. El t\'ermino $P(c)$ es conocido como probabilidad previa (\emph{prior probability}) y en caso de que la informaci\'on aportada por los t\'erminos no sea determinante en la selecci\'on podemos siempre escoger la clase con mayor valor de $P(c)$.
		
		Para simplificar a\'un mas el c\'omputo podemos sustituir los valores anteriores por sus logaritmos. Esto reducir'a el costo de hacer los c\'alculos y adem\'as los errores aritm\'eticos dado que la multiplicaci\'on se transforma en suma. La clase seleccionada ser\'ia entonces
		
		\[
				c_{map} = \argmax_{c\in\mathcal{C}} \left( \log(P(c))  + \sum_{1\leq k\leq n} \log(P(t_k\mid c)) \right).
		\]
		
		Solo nos queda ver como estimamos los par\'ametros $P(c)$ y $P (t_k\mid c)$ dado que los valores reales no son posibles de calcular.	Para la probabilidad previa podemos contar la frecuencia relativa de cada clase en $D$:
		
		\[P(c) = \frac{ N_c }{N} , \]
		donde $N_c$ es el n\'umero de documentos en la clase $c$ y $N$ es el numero total de documentos. Procedemos de manera similar para la probabilidad espec\'ifica de una palabra en una clase
		\[
			P(t_k\mid c) =\frac{ T_{c,t_k}}{T_{c}},
		\]
		donde $T_{c,t_k}$ indica la cantidad de veces que ocurre  la palabra $t_k$ en todos los documentos de la clase $c$ y $T_{c}$ es la cantidad total de palabras contando repeticiones) en toda la clase $c$. Si embargo, a\'un tenemos un problema con estas f\'ormulas y es que estamos asignando probabilidad cero a todos las clases que no contengan a todas las palabras del documento a clasificar. Para evitar esto adicionamos por defecto una unidad a cada contador lo cual es conocido como \emph{Laplace smoothing}
		\[
			P(t\mid c) = \frac{T_{c,t} + 1}{T_c + |V|},	
		\]
		donde $|V|$ es el n\'umero total de t\'eminos en el vocabulario.
		
		 Es importante destacar que en este m\'etodo estamos obviando la posici\'on de las palabras. Presentamos aqu\'i los algoritmos para entrenar y clasificar usando BN que fueron textualmente copiados de \color{red}{2009 Manning C. D., Introduction to Information Retrieval, p\'agina 260. [No recuerdo como citar esto correctamente. Also recordar que agregue unos paquetes arriba que no se si se agragan aqui o en otro de los .tex]}\color{black}.
		 
		 
		 
		 \begin{algorithm}{}
		 			\caption{TrainMultinomial}
		 	\begin{algorithmic}[1]
		 		
		 		% ENTRADA / SALIDA
		 		\Require{Set of classes $\mathcal{C}$ and training set $D$.} 
%		 		\Ensure{Trained.}
		 		\State{$ V \leftarrow ExtractVocabulary(D)$}
		 		\State{$ N \leftarrow CountDocs(D)$}
		 		\For{$c \in \mathcal{C}$}
		 		\State $N_c \leftarrow CountDocsInClass(D,c)$
		 		\State $prior[c] \leftarrow N_c/N$
		 		\State $text_c \leftarrow ConcatenateTextOfAllDocsInClass(D, c)$
		 		\For{$t \in V$}
		 		\State{$T_{ct} \leftarrow CountTokensOfTerm(text_c, t)$}
		 		\EndFor
		 		\For{$t \in V$}
		 		\State{$condprob[t][c] \leftarrow \frac{T_{c,t} + 1}{\sum_{t'} (T_{c,t'} + 1)}$}
		 		\EndFor 
		 		\EndFor
		 		\State \textbf{\Return} $V, prior, condprob$
		 	\end{algorithmic}
		 \end{algorithm}
		 
		 \begin{algorithm}
		 	\caption{ApplyMultinomialNB}
		 	\begin{algorithmic}[1]
		 		
		 		% ENTRADA / SALIDA
		 		\Require{$\mathcal{C}, V$, $prior$, $condprob$, $d$} 
		 		%		 			\Ensure{Trained.}
		 		\State{$ W \leftarrow ExtractTokensFromDoc(V, d)$}
		 		\For{$c \in \mathcal{C}$}
		 		\State{$ score[c] \leftarrow \log prior[c]$}
		 		\For{$t \in W$}
		 		\State{$ score[c] +=  \log condprob[t][c]$}
		 		\EndFor
		 		\EndFor
		 		\State \textbf{\Return} $\argmax_{c\in \mathcal{C}}(score[c])$
		 	\end{algorithmic}
		 \end{algorithm}
		 
		Podemos deducir de los algoritmos que la complejidad de ambos es linear en el tiempo que toma escanear la informaci\'on. Dado que esto hay que hacerlo al menos una vez, se puede decir que este m\'etodo tiene complejidad temporal \'optima. Dicha eficiencia hace que NB sea un m\'etodo de clasificaci\'on tan usado.
	\end{subsection}
	
	
	\begin{subsection}{Feature Selection}
		Un t\'ermino con ruido (\emph{noise feature}) es aquel que al pertenecer a la representaci\'on de los documentos, provoca un aumento del error de clasificaci\'on de los datos. Por ejemplo, supongamos que tenemos una palabra que ocurre rara vez, pero que en el conjunto de entrenamiento ocurre siempre en la misma clase. Entonces al clasificar un documento nuevo que contiene esta palabra, la misma provocar\'a que el clasificador se incline en cierta medida por seleccionar esta clase. Sin embargo, dado que la ocurrencia de esta palabra solo en la clase mencionada es accidental, claramente no aporta informaci\'on suficiente para la clasificaci\'on y por tanto, al considerarlo de otra forma, aumenta el error.
		
		Este es uno de los prop\'positos que tiene la selecci\'on de t\'erminos (\emph{feature selection} (FS)). Esta consiste en reducir el vocabulario, considerado en la clasificaci\'on de textos solo un subconjunto del que aparece en el conjunto de entrenamiento. N\'otese que al disminuir el tama\~no del vocabulario aumenta la eficiencia de los m\'etodos de entrenamiento y clasificaci\'on (aunque no es el caso de NB).
		
		Selecci\'on de t\'erminos prefiere un clasificador m\'as simple antes que uno m\'as complejo. Esto es \'util cuando el conjunto de entrenamiento no es muy grande.
		
%		Nos concentraremos en describir la Selecci\'on de t\'erminos para la clasificaci\'on de dos clases.

		 En FS usualmente fijamos una cantidad $k$ de vocablos por cada clase $c$, que ser\'an los usados por el clasificador. Para seleccionar los $k$ t\'erminos deseados establecemos un ranking entre los t\'erminos de la clase, haciendo uso de una funci\'on de medida de utilidad $A(t,c)$, y nos quedamos con los $k$ mejor posicionados. El algoritmo b\'asico consiste en para cada clase $c$ iterar por todos los t\'erminos del vocabulario y computar su medida de utilidad para la clase; para finalmente ordenar los resultados y devolver una lista con los $k$ mejores.
		 
		 Presentaremos a continuaci\'on tres de los m\'etodos de calcular $A(t,c)$ m\'as comunes.
		
		\begin{itemize}
			\item\textbf{Informaci\'on Manual.}
				Computar $A(t,c)$ como el valor esperado de informaci\'on mutua (\emph{Mutual Information} (MI)), nos da una medida de cuanta informaci\'on, la presencia en $c$ de un t\'ermino dado, aporta a tomar la decisi\'on correcta de clasificaci\'on de un documento. Lo definimos como \color{red}[Hay que poner aqui que esto se cogio del libro, pagina 272]\color{black}
				\[
					I(U_t;C_t) = \sum_{e_t \in \{ 1,0 \} } \sum_{e_c \in \{ 1,0 \} } P( U_t = e_t, C_t = e_c) \log_2 \frac{P (U_t = e_t, C_t = e_c) }{ P(U_t = e_t) P(C_t = e_c) },
				\]
				donde $U_t$ es una variable aleatoria que toma valor $e_t = 1$ si el documento contiene el t\'ermino $t$ y $e_t = 0$ en otro caso, y $C$ es otra variable aleatoria que toma valor $e_c = 1$ si el documento est\'a en la clase $c$ y $e_c = 0  $ en otro caso. 
	
				MI mide cu\'anta informaci\'on un t\'ermino contiene acerca de una clase. Por tanto mantener los t\'erminos que est\'an cargados de informaci\'on, y eliminar los que no, contribuye a reducir el ruido y mejorar la precisi\'on del clasificador.
	
			\item\textbf{Chi cuadrado $\chi^2$ FS.} En estad\'istica se dice que dos eventos son independientes si el resultado de uno no afecta al resultado del otro. Esto se puede escribir formalmente como $P(AB) = P(A) P(B)$. En estad\'istica el test $\chi^2$ se usa para medir el grado de independencia de dos eventos. En FS podemos entonces considerar aplicar este test asumiendo como eventos la ocurrencia de los t\'erminos y la ocurrencia de las clases. Esto es \color{red} Esto tambien hay que poner de donde lo cogi (pag 275)\color{black} 
			\[
				\chi^2(D,t,c) = \sum_{e_t\in \{ 1, 0 \}} \sum_{e_c\in \{ 1, 0 \}} \frac{(N_{e_te_c} - E_{e_t e_c}) ^2 } { E_{e_t e_c}},
			\]
			donde $N$ es la frecuencia seg\'un $D$, $E$ es la frecuencia esperada y $e_t$ y  $e_c$ se definen como en la medida anterior.
			
			\item \textbf{Selecci\'on basada en frecuencia}. Esta medida consiste en priorizar los t\'erminos que son m\'as comunes en la  clase. Puede ser calculado de dos formas diferentes. La primera es cantidad de repeticiones de un t\'ermino en los documentos de una clase, conocida como frecuencia en colecci\'on. La otra es frecuencia de documentos, y se calcula como la cantidad de documentos en la clase que contienen al t\'ermino en cuesti\'on.
			
			Cuando son seleccionados varios miles de t\'erminos, entonces esta medida es bastante buena. Esta es preferible a otros m\'etodos m\'as complejos cuando se aceptan soluciones sub\'optimas.
			
		\end{itemize}
	\end{subsection}
	
	\subsection{K Nearest Neighbor}
	
		En el algoritmo de Naive Bayes represent\'abamos los documentos como vectores booleanos de t\'erminos. Luego vimos que hay t\'erminos que no eran relevantes y que aportaban ruido, y lo solucionamos seleccionando para el clasificador solamente un subconjunto de todos los t\'erminos. A\'un as\'i estamos clasificando la relevancia de cada t\'ermino  en relevante o no relevante (o que introduce ruido).
		
		 El m\'etodo que presentamos en esta secci\'on, as\'i como otros similares, asignan a cada t\'ermino cierto valor de importancia relativa al documento en que aparece. Para esto se cambia la representaci\'on de los documentos a vectores de $\mathbb{R}^{|V|}$ donde a cada componente corresponde cierto peso que se le asigna al t\'ermino correspondiente a esa coordenada. Entonces, el espacio de documentos $\mathcal{X}$ (dominio de $\gamma$) es $\mathbb{R}^{|V|}$. A esta forma de representaci\'on de documentos se le conoce como modelo de espacio de vectores. La hip\'otesis b\'asica para usar el modelo de espacio de vectores es la siguiente \color{red} citar adecuadamente : pagina 289 \color{black}
		 
		 \textbf{Hip\'otesis de contig\"uidad:} Documentos en la misma clase forman una regi\'on contigua  y regiones de diferentes clases no se superponen.
		 
		 Las decisiones de muchos clasificadores basados en espacio de vectores dependen de una noci\'on de distancia. Pueden ser usadas por ejemplo similitud basado en el coseno (del \'angulo formado entre los vectores) o distancia Euclideana. Por lo general no hay mucha diferencia entre usar una u otra de estas distancias.
		 
		 La tarea de la clasificaci\'on en el modelo de espacio de vectores es determinar las fronteras entre los documentos pertenecientes a una u otra clase. Estas \'ultimas son llamadas fronteras de decisi\'on ya que estas dividen el espacio en diferentes hiperespacios, tales que si un documento pertenece a un hiperespacio determinado, autom\'aticamente sabemos de qu\'e clase es. En K Nearest Neighbor esta ...
	
\begin{itemize}
	\item Aprendizaje supervisado
	
	\item Problema que resuelve
\end{itemize}

\begin{itemize}
\item Rule-based classification

\item Statistical classification

\item Feature selection

\item Medidas de evaluaci\'on:
\begin{itemize}
	\item Fitting
	\item Precisi\'on
	\item Recobrado
	\item Medida F (balanceada)
	\item Classification accuracy
\end{itemize}

\item Algoritmos:
\begin{itemize}
	\item Naive Bayes
	
	
	\item K-Nearest Neighbours
\end{itemize}
\end{itemize}

\subsection{Aplicaciones a la RI}
\begin{itemize}
	\item Standing queries
	\item Spam filtering
\end{itemize}

\subsection{Ventajas} 

\subsection{Desventajas}

\section{Agrupamiento vs. Clasificaci\'on}

\section{Ejemplos de aplicación}

\section{Conclusiones}

\end{document}
