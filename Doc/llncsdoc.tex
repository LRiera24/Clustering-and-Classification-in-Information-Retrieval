% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{color}

\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage[noend]{algpseudocode}


\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[spanish]{babel}
%
\begin{document}
\markboth{Sistemas de Recuperaci\'on de Informaci\'on}{Aplicaciones del agrupamiento y de la clasificación en la RI en la Web}
\thispagestyle{empty}
\begin{flushleft}
\LARGE\bfseries Sistemas de Recuperaci\'on de Informaci\'on\\[2cm]
\end{flushleft}
\rule{\textwidth}{1pt}
\vspace{2pt}
\begin{flushright}
\Huge
\begin{tabular}{@{}l}
Aplicaciones del\\
agrupamiento y\\ 
de la clasificación\\ 
en la recuperaci\'on\\ 
de informaci\'on\\
en la Web\\[6pt]
\end{tabular}
\end{flushright}
\rule{\textwidth}{1pt}
\vfill
\begin{flushleft}
\large\itshape
\begin{tabular}{@{}l}
{\large\upshape\bfseries Autores}\\[8pt]
Laura Victoria Riera P\'erez\\[5pt]
Marcos Manuel Tirador del Riego
\end{tabular}
\end{flushleft}

%
\newpage
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}

\newpage
\pagenumbering{arabic}

\section{Agrupamiento}

\begin{itemize}
\item Aprendizaje no supervisado

\item Problema que resuelve
\end{itemize}

\subsection{Algunas definiciones}

\begin{itemize}
\item Flat clustering

\item Hierarchical clustering

\item Hard clustering

\item Soft clustering

\item Hip\'otesis de agrupamiento

\item Cardinalidad
\end{itemize}

\subsection{Flat clustering}

\begin{itemize}
\item Medida de similitud:

\item Medidas de evaluacion:
\begin{itemize}
	\item Criterio interno de calidad
	
	\item Criterio interno de calidad
	
	\item Pureza
	
	\item \'Indice de frontera?
	
	\item Medida F
\end{itemize}

\item Algoritmos:
\begin{itemize}
	\item K-means
	\item EM (generalizaci\'on de K-means)
\end{itemize}
\end{itemize}


\subsection{Hierarchical clustering}

\begin{itemize}
\item Hierarchical agglomerative clustering

\item Medidas de similitud:
\begin{itemize}
	\item Single link clustering
	\item Complete link clustering
	\item Centroid clustering
\end{itemize}

\item Evaluaci\'on de calidad:
\begin{itemize}
\item Group average link
\item M\'etodo de Ward
\end{itemize}

\item Divisive clustering

\item Cluster labeling

\item Algoritmos:
\begin{itemize}
	\item Algoritmo HAC
	\item Divisive Clustering
\end{itemize}
\end{itemize}

\subsection{Aplicaciones a la RI} 
\begin{itemize}
	\item Search result clustering
	
	\item Scatter-Gather
	
	\item Collection clustering
	
	\item Language modeling
	
	\item Cluster-based retrieval
\end{itemize}

\subsection{Ventajas} 

\subsection{Desventajas} 

\section{Clasificaci\'on}

	El problema de clasificaci\'on en sentido general consiste en determinar dentro un conjunto de clases a cu\'a de ellas pertenece un objeto dado. En el marco de este documento estamos interesados en estudiar la clasificaci\'on de textos. 
	
	La forma m\'as simple de clasificaci\'on de un conjunto de textos es la denominada clasificaci\'on en dos clases. Dichas clases est\'an determinadas por un t\'opico espec\'ifico y ser\'ian: \emph{documentos sobre dicho tema } y \emph{documentos no relacionados con el tema}. Este tipo de clasificaci\'on en ocasiones es llamada \emph{filtrado}. 
	
	La forma m\'as antigua de llevar a cabo la clasificaci\'on es manualmente. Por ejemplo, los bibliotecarios clasifican los libros de acuerdo a ciertos criterios, de modo que encontrar una informaci\'on buscada no resulte una tarea de gigante dificultad. Sin embargo la clasificaci\'on manual tiene sus l\'imites de escalabilidad. 
	
	Como alternativa podr\'ia pensarse el uso de \emph{reglas} para determinar autom\'aticamente si un texto pertenece o no a una colecci\'on de documentos relacionados con un tema. Por ejemplo las consultas permanentes son un ejemplo de regla aplicada autom\'aticamente. Una consulta permanente es como una consulta normal pero que es ejecutada repetidamente sobre una colecci\'on de textos a la cual se van adicionando documentos nuevos constantemente. Su finalidad es determinar si los nuevos textos pertenecen o no a la clase en cuesti\'on.
	
	Una regla captura una cierta combinaci\'on de palabras claves que identifican una clase. Reglas codificadas a mano pueden llegar a ser altamente escalable, pero crearlas y mantenerlas requiere un elevado costo en recursos humanos.
	
	Existe, sin embargo, un enfoque adicional a los dos anteriores mencionados. Nos referimos al uso de \emph{Aprendizaje de M\'aquinas}. En este enfoque el conjunto de reglas de clasificaci\'on, o en general, el criterio usado para clasificar, es aprendido de forma autom\'atica a partir de los datos de entrenamiento.
	
	Introduciremos a continuaci\'on la definici\'on forma del problema de clasificaci\'on de textos, en el contexto del Aprendizaje de M\'aquinas.
	
	\begin{definition} \label{Problema_de_clasificacion}
		Sea $\mathcal{{X}}$ el espacio de documentos y $\mathcal{C} := \{c_i \mid c_i \subset \mathcal{X}, i \in \{ 1,2,\dots,n\} \}$ un conjunto fijo de clases (tambi\'en llamadas categor\'ias o etiquetas). Sea adem\'as $D$ un conjunto entrenado de documentos clasificados $(d,c) \in \mathcal X \times \mathcal{C}$. El \emph{problema de la clasificaci\'on de textos} consiste en encontrar, usando m\'etodos o algoritmos de aprendizaje, una funci\'on \emph{clasificadora} $\gamma : \mathcal{X} \rightarrow \mathcal{C}$, que mapee documentos a clases, que satisfaga que $D \subset \gamma$. 	

	\end{definition}
	
	El aprendizaje que toma parte en la b\'usqueda de $\gamma$ es llamado \emph{aprendizaje supervisado} debido a que se necesita la ayuda de uno o varios expertos que creen el conjunto de entrenamiento $D$. Estos expertos son  tambi\'en quienes determinan el conjunto de clases en que se clasificar\'an los textos. Denotaremos el m\'etodo de aprendizaje supervisado descrito por $\Gamma$, el cual act\'ua como una funci\'on que mapea un conjunto de datos de entrenamiento en una funci\'on clasificadora, osea que $\Gamma(D) = \gamma$.
	
	La definici\'on dada en \ref{Problema_de_clasificacion} implica que cada documento pertenece a una sola clase. Pero existe otro tipo de problemas que permiten que un documento pertenezca a m\'as de una clase. Por ahora enfocaremos nuestra atenci\'on en el tipo de una clase.
	
	\begin{subsection}{Naive Bayes}

		Uno de los m\'etodos m\'as comunes de aprendizaje supervisado es el conocido como \emph{Naive Bayes} (NB). Este es un m\'etodo de aprendizaje probabil\'istico. La probabilidad de un documento $d$ de pertenecer a una clase $c$ se puede expresar como $P(c\mid d)$. La tarea del algoritmo es encontrar la mejor clase para cada documento $d$. Para ello NB establece que la clase m\'as apropiada para un documento es la m\'as probable, o sea
		
		\[
		c_{map} = \argmax_{c\in\mathcal{C}} P(c \mid d).
		\]
		
		La clase escogida para $d$ se denota por $c_{map}$ debido a que este m\'etodo de clasificaci\'on, de acuerdo a la clase m\'as probable para un documento dado, es conocido como \emph{maximum a posteriori} (MAP).
		
		Sin embargo la probabilidad $P(c \mid d)$ es dif\'icil de determinar. Haciendo uso del \emph{Teorema de Bayes} podemos expresarla como
		
		\[
		P(c \mid d) =\frac{ P(d\mid c) P(c)}{P(d)}.
		\]
		
		El factor de normalizaci\'on $P(d)$ es usualmente ignorado ya que no aporta informaci\'on a la hora de buscar la clase m\'as apropiada para un documento $d$, ya que este tiene el mismo efecto en todos los candidatos. Este c\'alculo puede ser simplificado lo expresamos en t\'erminos de los t\'erminos en los documentos. Supongamos que $\{t_1, t_2, \dots , t_n \}$ son los t\'erminos que aparecen en $d$. Entonces tenemos que  
		
		\[
			c_{map} = \argmax_{c\in\mathcal{C}} P(c) P(d \mid c) = \argmax_{c\in\mathcal{C}} P(c) \prod_{1\leq k\leq n} P(t_k \mid c),
		\]
		donde $P(t_k \mid c)$ es la probabilidad de que el t\'ermino $t_k$ aparezca en un documento de la clase $c$. Podemos considerar $p(t_k \mid c)$ como una medida de qu\'e tanto demuestra el t\'ermino $t_k$ que $c$ es la clase correcta. El t\'ermino $P(c)$ es conocido como probabilidad previa (\emph{prior probability}) y en caso de que la informaci\'on aportada por los t\'erminos no sea determinante en la selecci\'on podemos siempre escoger la clase con mayor valor de $P(c)$.
		
		Para simplificar a\'un mas el c\'omputo podemos sustituir los valores anteriores por sus logaritmos. Esto reducir'a el costo de hacer los c\'alculos y adem\'as los errores aritm\'eticos dado que la multiplicaci\'on se transforma en suma. La clase seleccionada ser\'ia entonces
		
		\[
				c_{map} = \argmax_{c\in\mathcal{C}} \left( \log(P(c))  + \sum_{1\leq k\leq n} \log(P(t_k\mid c)) \right).
		\]
		
		Solo nos queda ver como estimamos los par\'ametros $P(c)$ y $P (t_k\mid c)$ dado que los valores reales no son posibles de calcular.	Para la probabilidad previa podemos contar la frecuencia relativa de cada clase en $D$:
		
		\[P(c) = \frac{ N_c }{N} , \]
		donde $N_c$ es el n\'umero de documentos en la clase $c$ y $N$ es el numero total de documentos. Procedemos de manera similar para la probabilidad espec\'ifica de una palabra en una clase
		\[
			P(t_k\mid c) =\frac{ T_{c,t_k}}{T_{c}},
		\]
		donde $T_{c,t_k}$ indica la cantidad de veces que ocurre  la palabra $t_k$ en todos los documentos de la clase $c$ y $T_{c}$ es la cantidad total de palabras contando repeticiones) en toda la clase $c$. Si embargo, a\'un tenemos un problema con estas f\'ormulas y es que estamos asignando probabilidad cero a todos las clases que no contengan a todas las palabras del documento a clasificar. Para evitar esto adicionamos por defecto una unidad a cada contador lo cual es conocido como \emph{Laplace smoothing}
		\[
			P(t\mid c) = \frac{T_{c,t} + 1}{T_c + |V|},	
		\]
		donde $|V|$ es el n\'umero total de t\'eminos en el vocabulario.
		
		 Es importante destacar que en este m\'etodo estamos obviando la posici\'on de las palabras. Presentamos aqu\'i los algoritmos para entrenar y clasificar usando BN que fueron textualmente copiados de \color{red}{2009 Manning C. D., Introduction to Information Retrieval, p\'agina 260. [No recuerdo como citar esto correctamente. Also recordar que agregue unos paquetes arriba que no se si se agragan aqui o en otro de los .tex]}\color{black}.
		 
		 
		 
		 \begin{algorithm}{}
		 			\caption{TrainMultinomial}
		 	\begin{algorithmic}[1]
		 		
		 		% ENTRADA / SALIDA
		 		\Require{Set of classes $\mathcal{C}$ and training set $D$.} 
%		 		\Ensure{Trained.}
		 		\State{$ V \leftarrow ExtractVocabulary(D)$}
		 		\State{$ N \leftarrow CountDocs(D)$}
		 		\For{$c \in \mathcal{C}$}
		 		\State $N_c \leftarrow CountDocsInClass(D,c)$
		 		\State $prior[c] \leftarrow N_c/N$
		 		\State $text_c \leftarrow ConcatenateTextOfAllDocsInClass(D, c)$
		 		\For{$t \in V$}
		 		\State{$T_{ct} \leftarrow CountTokensOfTerm(text_c, t)$}
		 		\EndFor
		 		\For{$t \in V$}
		 		\State{$condprob[t][c] \leftarrow \frac{T_{c,t} + 1}{\sum_{t'} (T_{c,t'} + 1)}$}
		 		\EndFor 
		 		\EndFor
		 		\State \textbf{\Return} $V, prior, condprob$
		 	\end{algorithmic}
		 \end{algorithm}
		 
		 \begin{algorithm}
		 	\caption{ApplyMultinomialNB}
		 	\begin{algorithmic}[1]
		 		
		 		% ENTRADA / SALIDA
		 		\Require{$\mathcal{C}, V$, $prior$, $condprob$, $d$} 
		 		%		 			\Ensure{Trained.}
		 		\State{$ W \leftarrow ExtractTokensFromDoc(V, d)$}
		 		\For{$c \in \mathcal{C}$}
		 		\State{$ score[c] \leftarrow \log prior[c]$}
		 		\For{$t \in W$}
		 		\State{$ score[c] +=  \log condprob[t][c]$}
		 		\EndFor
		 		\EndFor
		 		\State \textbf{\Return} $\argmax_{c\in \mathcal{C}}(score[c])$
		 	\end{algorithmic}
		 \end{algorithm}
		 
		Podemos deducir de los algoritmos que la complejidad de ambos es linear en el tiempo que toma escanear la informaci\'on. Dado que esto hay que hacerlo al menos una vez, se puede decir que este m\'etodo tiene complejidad temporal \'optima. Dicha eficiencia hace que NB sea un m\'etodo de clasificaci\'on tan usado.
	\end{subsection}
	
	
	\begin{subsection}{Feature Selection}
		Un t\'ermino con ruido (\emph{noise feature}) es aquel que al pertenecer a la representaci\'on de los documentos, provoca un aumento del error de clasificaci\'on de los datos. Por ejemplo, supongamos que tenemos una palabra que ocurre rara vez, pero que en el conjunto de entrenamiento ocurre siempre en la misma clase. Entonces al clasificar un documento nuevo que contiene esta palabra, la misma provocar\'a que el clasificador se incline en cierta medida por seleccionar esta clase. Sin embargo, dado que la ocurrencia de esta palabra solo en la clase mencionada es accidental, claramente no aporta informaci\'on suficiente para la clasificaci\'on y por tanto, al considerarlo de otra forma, aumenta el error.
		
		Este es uno de los prop\'positos que tiene la selecci\'on de t\'erminos (\emph{feature selection} (FS)). Esta consiste en reducir el vocabulario, considerado en la clasificaci\'on de textos solo un subconjunto del que aparece en el conjunto de entrenamiento. N\'otese que al disminuir el tama\~no del vocabulario aumenta la eficiencia de los m\'etodos de entrenamiento y clasificaci\'on (aunque no es el caso de NB).
		
		Selecci\'on de t\'erminos prefiere un clasificador m\'as simple antes que uno m\'as complejo. Esto es \'util cuando el conjunto de entrenamiento no es muy grande.
		
%		Nos concentraremos en describir la Selecci\'on de t\'erminos para la clasificaci\'on de dos clases.

		 En FS usualmente fijamos una cantidad $k$ de vocablos por cada clase $c$, que ser\'an los usados por el clasificador. Para seleccionar los $k$ t\'erminos deseados establecemos un ranking entre los t\'erminos de la clase, haciendo uso de una funci\'on de medida de utilidad $A(t,c)$, y nos quedamos con los $k$ mejor posicionados. El algoritmo b\'asico consiste en para cada clase $c$ iterar por todos los t\'erminos del vocabulario y computar su medida de utilidad para la clase; para finalmente ordenar los resultados y devolver una lista con los $k$ mejores.
		 
		 Presentaremos a continuaci\'on tres de los m\'etodos de calcular $A(t,c)$ m\'as comunes.
		
		\begin{itemize}
			\item\textbf{Informaci\'on Manual.}
				Computar $A(t,c)$ como el valor esperado de informaci\'on mutua (\emph{Mutual Information} (MI)), nos da una medida de cuanta informaci\'on, la presencia en $c$ de un t\'ermino dado, aporta a tomar la decisi\'on correcta de clasificaci\'on de un documento. Lo definimos como \color{red}[Hay que poner aqui que esto se cogio del libro, pagina 272]\color{black}
				\[
					I(U_t;C_t) = \sum_{e_t \in \{ 1,0 \} } \sum_{e_c \in \{ 1,0 \} } P( U_t = e_t, C_t = e_c) \log_2 \frac{P (U_t = e_t, C_t = e_c) }{ P(U_t = e_t) P(C_t = e_c) },
				\]
				donde $U_t$ es una variable aleatoria que toma valor $e_t = 1$ si el documento contiene el t\'ermino $t$ y $e_t = 0$ en otro caso, y $C$ es otra variable aleatoria que toma valor $e_c = 1$ si el documento est\'a en la clase $c$ y $e_c = 0  $ en otro caso. 
	
				MI mide cu\'anta informaci\'on un t\'ermino contiene acerca de una clase. Por tanto mantener los t\'erminos que est\'an cargados de informaci\'on, y eliminar los que no, contribuye a reducir el ruido y mejorar la precisi\'on del clasificador.
	
			\item\textbf{Chi cuadrado $\chi^2$ FS.} En estad\'istica se dice que dos eventos son independientes si el resultado de uno no afecta al resultado del otro. Esto se puede escribir formalmente como $P(AB) = P(A) P(B)$. En estad\'istica el test $\chi^2$ se usa para medir el grado de independencia de dos eventos. En FS podemos entonces considerar aplicar este test asumiendo como eventos la ocurrencia de los t\'erminos y la ocurrencia de las clases. Esto es \color{red} Esto tambien hay que poner de donde lo cogi (pag 275)\color{black} 
			\[
				\chi^2(D,t,c) = \sum_{e_t\in \{ 1, 0 \}} \sum_{e_c\in \{ 1, 0 \}} \frac{(N_{e_te_c} - E_{e_t e_c}) ^2 } { E_{e_t e_c}},
			\]
			donde $N$ es la frecuencia seg\'un $D$, $E$ es la frecuencia esperada y $e_t$ y  $e_c$ se definen como en la medida anterior.
			
			\item \textbf{Selecci\'on basada en frecuencia}. Esta medida consiste en priorizar los t\'erminos que son m\'as comunes en la  clase. Puede ser calculado de dos formas diferentes. La primera es cantidad de repeticiones de un t\'ermino en los documentos de una clase, conocida como frecuencia en colecci\'on. La otra es frecuencia de documentos, y se calcula como la cantidad de documentos en la clase que contienen al t\'ermino en cuesti\'on.
			
			Cuando son seleccionados varios miles de t\'erminos, entonces esta medida es bastante buena. Esta es preferible a otros m\'etodos m\'as complejos cuando se aceptan soluciones sub\'optimas.
			
		\end{itemize}
	\end{subsection}
	
	\subsection{K Nearest Neighbor}
	
		En el algoritmo de Naive Bayes represent\'abamos los documentos como vectores booleanos de t\'erminos. Luego vimos que hay t\'erminos que no eran relevantes y que aportaban ruido, y lo solucionamos seleccionando para el clasificador solamente un subconjunto de todos los t\'erminos. A\'un as\'i estamos clasificando la relevancia de cada t\'ermino  en relevante o no relevante (o que introduce ruido).
		
		 El m\'etodo que presentamos en esta secci\'on, as\'i como otros similares, asignan a cada t\'ermino cierto valor de importancia relativa al documento en que aparece. Para esto se cambia la representaci\'on de los documentos a vectores de $\mathbb{R}^{|V|}$ donde a cada componente corresponde cierto peso que se le asigna al t\'ermino correspondiente a esa coordenada. Entonces, el espacio de documentos $\mathcal{X}$ (dominio de $\gamma$) es $\mathbb{R}^{|V|}$. A esta forma de representaci\'on de documentos se le conoce como modelo de espacio de vectores. La hip\'otesis b\'asica para usar el modelo de espacio de vectores es la siguiente \color{red} citar adecuadamente : pagina 289 \color{black}
		 
		 \textbf{Hip\'otesis de contig\"uidad:} Documentos en la misma clase forman una regi\'on contigua  y regiones de diferentes clases no se superponen.
		 
		 Las decisiones de muchos clasificadores basados en espacio de vectores dependen de una noci\'on de distancia. Pueden ser usadas por ejemplo similitud basado en el coseno (del \'angulo formado entre los vectores) o distancia Euclideana. Por lo general no hay mucha diferencia entre usar una u otra de estas distancias.
		 
		 La tarea de la clasificaci\'on en el modelo de espacio de vectores es determinar las fronteras entre los documentos pertenecientes a una u otra clase. Estas \'ultimas son llamadas fronteras de decisi\'on ya que estas dividen el espacio en diferentes hiperespacios, tales que si un documento pertenece a un hiperespacio determinado, autom\'aticamente sabemos de qu\'e clase es. En K Nearest Neighbor esta ...
	
\begin{itemize}
	\item Aprendizaje supervisado
	
	\item Problema que resuelve
\end{itemize}

\begin{itemize}
\item Rule-based classification

\item Statistical classification

\item Feature selection

\item Medidas de evaluaci\'on:
\begin{itemize}
	\item Fitting
	\item Precisi\'on
	\item Recobrado
	\item Medida F (balanceada)
	\item Classification accuracy
\end{itemize}

\item Algoritmos:
\begin{itemize}
	\item Naive Bayes
	
	
	\item K-Nearest Neighbours
\end{itemize}
\end{itemize}

\subsection{Aplicaciones a la RI}
\begin{itemize}
	\item Standing queries
	\item Spam filtering
\end{itemize}

\subsection{Ventajas} 

\subsection{Desventajas}

\section{Agrupamiento vs. Clasificaci\'on}

\section{Ejemplos de aplicación}



\end{document}
